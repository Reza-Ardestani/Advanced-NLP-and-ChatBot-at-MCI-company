{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_IDF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drKiwgo7QMoc",
        "outputId": "47546506-6f8a-478d-8a26-db6f05413147"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3LFIb_4G3ql"
      },
      "source": [
        "### This video covers:\n",
        "\n",
        "\n",
        "*   Implementation of TF-IDF representation\n",
        "*   Word2vec representation by gensim library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJppNRwPIZBy"
      },
      "source": [
        "### part 1: Implementation of TF-IDF representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhzMxF5DHJ34"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1F4gXB_YhEKyJrQjtZ5e7UvzapsfhW1lR)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1lU5MBuk7Ap"
      },
      "source": [
        "# this function reads the 'Hamshahri' corpus and returns its documents (docs) and document Ids (DIDs)\n",
        "def read_document(path):\n",
        "\n",
        "  res = []\n",
        "\n",
        "  with open(path, 'r', encoding = 'utf-8') as ptr:\n",
        "    for line in ptr:\n",
        "      res.append(line)\n",
        "\n",
        "  tmp = ' '\n",
        "  docs = []\n",
        "  DIDs = []\n",
        "  DIDs.append(res[0].split()[1])\n",
        "\n",
        "  for i in range(1, len(res)):\n",
        "\n",
        "    if res[i].startswith(\".DID\"):\n",
        "      docs.append(tmp)\n",
        "      tmp = ''\n",
        "      DIDs.append(res[i].split()[1])\n",
        "      continue\n",
        "\n",
        "    if res[i].startswith(\".Date\"):\n",
        "      continue\n",
        "\n",
        "    if res[i].startswith(\".Cat\"):\n",
        "      continue\n",
        "\n",
        "    tmp = tmp+ ' ' + res[i].strip()\n",
        "\n",
        "  docs.append(tmp)\n",
        "\n",
        "  return docs, DIDs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQwE0_sEmzt4"
      },
      "source": [
        "# this function removes the punctuation marks and non-alphabetic words from given document \n",
        "# and returns an array of document words\n",
        "def clean_text(doc):\n",
        "\n",
        "  tokens = doc.split()\n",
        "  translation_table = str.maketrans('', '', \"><.،؟؛:{}\\|+ـ()*&^٪$#❊!/[]=-\")\n",
        "  tokens = [word.translate(translation_table) for word in tokens]\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqZ41prEl9an"
      },
      "source": [
        "docs, DIDs = read_document('/content/drive/MyDrive/dataset/Hamshahri-Corpus.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81WZ9XuAuwJw"
      },
      "source": [
        "N = len(docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XhLor3wxEgW"
      },
      "source": [
        "\n",
        "# this function creates two term_doc, and doc_term dictionaries\n",
        "def create_tf_idf_dicts(docs, DIDs):\n",
        "  \n",
        "  # [key = T (term)] --> dict[key = D (doc)] --> number of times the term T appeared in doc D\n",
        "  term_doc = {}\n",
        "\n",
        "  # [key = D (doc)] --> dict[key = T (term)] --> number of times the term T appeared in doc D\n",
        "  doc_term = {}\n",
        "\n",
        "  for doc, DID in zip(docs, DIDs):\n",
        "    terms = clean_text(doc)\n",
        "    doc_term[DID] = {}\n",
        "\n",
        "    for term in terms:\n",
        "   \n",
        "  # update doc_term[DID] dictionary\n",
        "\n",
        "      # current term has appeared in document 'DID' in orevious steps:\n",
        "      if term in doc_term[DID]:\n",
        "        doc_term[DID][term] += 1\n",
        "      \n",
        "      # this is the first occurrance of the current term in document 'DID':\n",
        "      else: \n",
        "        doc_term[DID][term] = 1\n",
        "\n",
        "  # update term_doc[term] dictionary\n",
        "\n",
        "      if not term in term_doc:\n",
        "        term_doc[term] = {}\n",
        "\n",
        "      # current term has appeared in document DID in previous steps:\n",
        "      if DID in term_doc[term]:\n",
        "          term_doc[term][DID] +=1\n",
        "      \n",
        "      # this is the first time the current term appears in document DID:\n",
        "      else: \n",
        "        term_doc[term][DID] = 1     \n",
        "\n",
        "  return term_doc, doc_term"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_Ih9ZDroUFj"
      },
      "source": [
        "term_doc, doc_term = create_tf_idf_dicts(docs, DIDs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjgERWV9zVtA"
      },
      "source": [
        "import math\n",
        "\n",
        "# this function calculates the TF-IDF weight of input term within the input document\n",
        "def get_tf_idf(term, doc):\n",
        "  \n",
        "  count_t_d = 0\n",
        "  \n",
        "  if doc in term_doc[term]:\n",
        "    count_t_d = term_doc[term][doc]\n",
        "    tf = 1 + math.log(count_t_d, 10)\n",
        "  \n",
        "  idf = get_IDF(term)\n",
        "\n",
        "  return tf*idf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlr2US-43xaJ"
      },
      "source": [
        "# this function calculates the IDF weight of input term\n",
        "def get_IDF(term):\n",
        "  \n",
        "  df = 0\n",
        "  \n",
        "  for doc in doc_term:\n",
        "    if term in doc_term[doc]:\n",
        "      df += 1\n",
        "  \n",
        "  if df>0:\n",
        "    idf = math.log((N/df), 2)\n",
        "  \n",
        "  return idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOsT9UGItSl5",
        "outputId": "ba371283-a8a8-4090-cf17-13bfdc1cc10d"
      },
      "source": [
        "get_IDF('تهران')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.8190675501480003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUHZNjov2-WG"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1BpsCwRd2cee7lGHWXarOrtCxKu8a-f3B)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzkUL_Kv31IG"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# this function claculates the cosine similarity of the input documents \n",
        "def cosine_sim_of_docs(doc_1, doc_2):\n",
        "  \n",
        "  if not (doc_1 in doc_term and doc_2 in doc_term):\n",
        "    print('Invalid document number')\n",
        "    return 0\n",
        "\n",
        "  dot_product = 0\n",
        "  \n",
        "  tf_doc_1 = doc_term[doc_1]\n",
        "  tf_doc_2 = doc_term[doc_2]\n",
        "\n",
        "  # compute the dot product of two document representations\n",
        "  for term in tf_doc_1:   \n",
        "    if term in tf_doc_2:\n",
        "\n",
        "      dot_product += get_tf_idf(term, doc_1) *  get_tf_idf(term, doc_2) \n",
        "  \n",
        "  length_doc_1 = 0\n",
        "  length_doc_2 = 0\n",
        "\n",
        "  # compute the length of doc_1's representation\n",
        "  for term in tf_doc_1:\n",
        "    length_doc_1 += get_tf_idf(term, doc_1) * get_tf_idf(term, doc_1)\n",
        "\n",
        "  # compute the length of doc_2's representation\n",
        "  for term in tf_doc_2:\n",
        "    length_doc_2 += get_tf_idf(term, doc_2) * get_tf_idf(term, doc_2)\n",
        "\n",
        "  # calculate the cosine similarity od doc_1 and doc_2\n",
        "  cosine_sim = dot_product / (np.sqrt(length_doc_1) * np.sqrt(length_doc_2))\n",
        "\n",
        "  return cosine_sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKkoshogRZqg",
        "outputId": "3a235706-6949-41e2-945c-4cee6bf5e713"
      },
      "source": [
        "cosine_sim_of_docs('244S1', '279S1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01656446971653419"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD6O4GfxSAAU"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# this function claculates the cosine similarity of the input term \n",
        "def cosine_sim_of_words(term_1, term_2):\n",
        "\n",
        "  if not (term_1 in term_doc and term_2 in term_doc):\n",
        "    print('out of vocab word!')\n",
        "    return 0\n",
        "\n",
        "  dot_product = 0\n",
        "  \n",
        "  doc_freq_term_1 = term_doc[term_1]\n",
        "  doc_freq_term_2 = term_doc[term_2]\n",
        "\n",
        "  idf_term_1 = get_IDF(term_1)\n",
        "  idf_term_2 = get_IDF(term_2)\n",
        "  \n",
        "  # compute the dot product of two term representations\n",
        "  for doc in doc_freq_term_1:   \n",
        "    if doc in doc_freq_term_2:\n",
        "      dot_product += get_tf_idf(term_1, doc) * get_tf_idf(term_2, doc) \n",
        "  \n",
        "  length_doc_1 = 0\n",
        "  length_doc_2 = 0\n",
        "\n",
        "\n",
        "  # compute the length of term_1's representation\n",
        "  for doc in doc_freq_term_1:\n",
        "    length_doc_1 += get_tf_idf(term_1, doc) * get_tf_idf(term_1, doc)\n",
        "\n",
        "  # compute the length of term_2's representation\n",
        "  for doc in doc_freq_term_2:\n",
        "    length_doc_2 += get_tf_idf(term_2, doc) * get_tf_idf(term_2, doc)\n",
        "\n",
        "  # calculate the cosine similarity od term_1 and term_2\n",
        "  cosine_sim = dot_product / (np.sqrt(length_doc_1) * np.sqrt(length_doc_2))\n",
        "\n",
        "  return cosine_sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZweJhRo2Iji3"
      },
      "source": [
        "### part 2: Word2vec by Gensim\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voHGSExomE39"
      },
      "source": [
        "# create input for gensim word2vec model\n",
        "\n",
        "sentences = []\n",
        "for doc in docs:\n",
        "  sentences.append(clean_text(doc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeE3FMcrpgt9"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# sg: Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
        "# min_count: Ignores all words with total frequency lower than this.\n",
        "# window: Maximum distance between the current and predicted word within a sentence.\n",
        "# size: Dimensionality of the word vectors.\n",
        "\n",
        "model = Word2Vec(sentences, min_count=1, size=100, window=10, sg=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tRNT_xRu1Hd"
      },
      "source": [
        "# save the model\n",
        "model.save(\"/content/drive/MyDrive/models/word2vec.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjcOkplBEJBd"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "# load the model\n",
        "model = Word2Vec.load(\"/content/drive/MyDrive/models/word2vec.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbMY9EZSuyAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8134149-f71d-4e8c-842a-b926c9ec3843"
      },
      "source": [
        "# train the model\n",
        "model.train([[\"همشهری\", \"روزنامه\"]], total_examples=1, epochs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yydV8p-Nu4jn"
      },
      "source": [
        "# get vector representation of each term\n",
        "vector = model.wv['خيابان'] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0IAIphsFM_E",
        "outputId": "d80c901c-b6d4-4b01-d76f-c4a7f9350236"
      },
      "source": [
        "vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4.7829500e-01, -3.3936438e-01,  8.2576412e-01,  3.7145108e-01,\n",
              "        6.7576092e-01,  5.8686972e-02,  4.8742581e-02,  1.2406912e-01,\n",
              "       -4.3891722e-01,  7.0365928e-02, -3.4964731e-01, -1.3779813e-01,\n",
              "        1.6468327e-01,  2.4502856e-01,  3.5093248e-01, -3.2618856e-01,\n",
              "        5.9894842e-01,  1.0751355e+00,  1.6326882e-02, -1.5088303e-01,\n",
              "        1.6317840e-01, -1.2954479e-01,  3.6789209e-01, -5.5537474e-01,\n",
              "        1.9381578e-01, -1.3732421e-01, -3.4485805e-01,  1.7315307e-01,\n",
              "        3.7723914e-01, -6.6332656e-01,  3.6547026e-01,  3.6495346e-01,\n",
              "        2.6299605e-01,  2.7699494e-01,  5.5166113e-01, -3.7951469e-02,\n",
              "        2.4369428e-02, -1.6798066e-01,  4.6101886e-01, -3.3071759e-01,\n",
              "       -8.0350757e-01, -1.4511347e-01, -4.4636783e-01, -8.4169728e-01,\n",
              "        3.7407890e-01, -6.7757732e-01,  3.2781810e-02,  1.1740174e-01,\n",
              "       -3.2059025e-02,  2.0361900e-01,  1.8284015e-02,  5.0743729e-01,\n",
              "       -7.9127812e-01,  1.4968026e-01, -3.1331035e-01,  6.5365329e-02,\n",
              "        2.8309339e-01,  1.2984358e-01, -3.6233533e-02, -1.2127010e-02,\n",
              "        5.6021649e-01,  4.0005535e-01,  3.3604065e-01, -3.9629486e-01,\n",
              "        4.0788096e-01, -1.4861806e-01,  2.9002714e-01, -8.6864299e-01,\n",
              "        8.8348401e-01,  2.9378653e-01,  2.3446381e-01, -1.9823499e-01,\n",
              "       -1.2798081e-03,  3.5602266e-01, -6.1617970e-01,  1.6538659e-01,\n",
              "        5.4843850e-02,  2.8382158e-01,  2.2982329e-01,  5.2924216e-01,\n",
              "        4.5817807e-01, -9.2289150e-02,  5.4847383e-01,  7.2766954e-01,\n",
              "        1.4930275e-01,  1.4327475e-01,  4.1433772e-01, -4.6661693e-01,\n",
              "        8.5197367e-02,  5.9736794e-01,  9.4620623e-02, -1.5131103e-01,\n",
              "       -1.2676959e-01,  7.9920685e-01, -4.1024169e-01,  2.7905273e-01,\n",
              "       -3.1543005e-01,  1.0854067e-02, -8.6887115e-01, -1.4068953e+00],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWjU4Y7r8F3-",
        "outputId": "96da46ea-d3c4-4e9d-bc36-5b3c60fa6d6b"
      },
      "source": [
        "# get the most similar term to the input term\n",
        "sims = model.wv.most_similar('خيابان', topn=5)\n",
        "print(sims)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('چهارراه', 0.8367165327072144), ('وليعصر', 0.8330073356628418), ('كوچه', 0.8281815052032471), ('ميرداماد', 0.8225080370903015), ('ضلع', 0.8036145567893982)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLwsu7GZ8HhS",
        "outputId": "dd2d0e83-2467-454a-87e5-084928474d06"
      },
      "source": [
        "# get the cosine similarity of TF-IDF representations\n",
        "\n",
        "print(cosine_sim_of_words('خيابان', 'كوچه'))\n",
        "print(cosine_sim_of_words('خيابان', 'چهارراه'))\n",
        "print(cosine_sim_of_words('خيابان', 'وليعصر'))\n",
        "print(cosine_sim_of_words('خيابان', 'مطهري'))\n",
        "print(cosine_sim_of_words('خيابان', 'تقاطع'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.37874202264156015\n",
            "0.287817732181497\n",
            "0.2704187837993078\n",
            "0.25563191693756854\n",
            "0.19136493917027125\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}