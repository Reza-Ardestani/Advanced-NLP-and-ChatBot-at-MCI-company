{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raAXuhfstqj8"
   },
   "source": [
    "# Functions/Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "yAUjqTvOhM0W",
    "outputId": "175f5e80-8a61-4d5c-ffac-0753b00cc813"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nPersian Preprocessing tools:\\nhttps://github.com/ICTRC/Parsivar\\nhttps://github.com/Dadmatech\\nhttps://github.com/sobhe/hazm/\\nhttps://github.com/mhbashari/awesome-persian-nlp-ir/blob/master/sections/tools.md\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Persian Preprocessing tools:\n",
    "https://github.com/ICTRC/Parsivar\n",
    "https://github.com/Dadmatech\n",
    "https://github.com/sobhe/hazm/\n",
    "https://github.com/mhbashari/awesome-persian-nlp-ir/blob/master/sections/tools.md\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "0cPOV2aItp8m"
   },
   "outputs": [],
   "source": [
    "# get rid of spacy and useless outputs \n",
    "from IPython.display import clear_output as cls\n",
    "\n",
    "# for FarsiParsi\n",
    "!pip install hazm \n",
    "!pip install parsivar\n",
    "#!pip install dadmatools \n",
    "\n",
    "import hazm\n",
    "import parsivar\n",
    "#import dadmatools\n",
    "\n",
    "# For reading csv files\n",
    "import pandas as pd \n",
    "\n",
    "# For finding minimum edit distance\n",
    "!pip install weighted-levenshtein\n",
    "import numpy as np\n",
    "from weighted_levenshtein import lev, osa, dam_lev\n",
    "\n",
    "cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "iWwLLXBy8W5D"
   },
   "outputs": [],
   "source": [
    "#import hazm\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "class FarsiParsi:\n",
    "  # Farsi Parser :) \n",
    "  def __init__(self,):\n",
    "    self.normalize_func = hazm.Normalizer(affix_spacing=False,\n",
    "                                          persian_numbers=False,\n",
    "                                          persian_style=False).normalize\n",
    "    self.word_tokenizer = hazm.word_tokenize\n",
    "    st_words = hazm.stopwords_list()\n",
    "    st_words.append('ی');st_words.append('های')\n",
    "    self.stops = {}\n",
    "    for k in st_words:\n",
    "      self.stops[k] = False\n",
    "\n",
    "\n",
    "  def clean_tokenize(self,string):\n",
    "    d ='0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    norm_txt = self.normalize_func(string)\n",
    "    norm_txt = re.sub(f'[{d}{punctuation}؟،٪×÷»«؛]+', '', norm_txt)\n",
    "    tokens = hazm.word_tokenize(norm_txt)\n",
    "    res = [t for t in tokens if self.stops.get(t, True) and len(t)>2]\n",
    "    return res\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "M1MMLIoh8hNi"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from weighted_levenshtein import lev, osa, dam_lev\n",
    "# Ref: https://pypi.org/project/weighted-levenshtein/\n",
    "# Ref: https://www.geeksforgeeks.org/edit-distance-dp-5/\n",
    "\n",
    "class SpellCorrection:\n",
    "\n",
    "  def __init__(self, insert_cost= 1 ,delete_cost = 1,\n",
    "               substitute_cost = 2, ) -> None:\n",
    "      in_costs = np.ones(128, dtype=np.float64)\n",
    "      dl_costs = np.ones(128, dtype=np.float64)  \n",
    "      sb_costs = np.ones((128, 128), dtype=np.float64)  \n",
    "      in_costs[in_costs > -1] = insert_cost; self.in_costs = in_costs\n",
    "      dl_costs[dl_costs > -1] = delete_cost; self.dl_costs = dl_costs\n",
    "      sb_costs[sb_costs > -1] = substitute_cost; self.sb_costs = sb_costs\n",
    "\n",
    "  def _editDistance(self,str1, str2, m, n):\n",
    "    if m == 0:\n",
    "      return n\n",
    "\n",
    "    # If second string is empty, the only option is to\n",
    "    # remove all characters of first string\n",
    "    if n == 0:\n",
    "      return m\n",
    "\n",
    "    # If last characters of two strings are same, nothing\n",
    "    # much to do. Ignore last characters and get count for\n",
    "    # remaining strings.\n",
    "    if str1[m-1] == str2[n-1]:\n",
    "      return self._editDistance(str1, str2, m-1, n-1)\n",
    "\n",
    "    # If last characters are not same, consider all three\n",
    "    # operations on last character of first string, recursively\n",
    "    # compute minimum cost for all three operations and take\n",
    "    # minimum of three values.\n",
    "    return 1 + min(self._editDistance(str1, str2, m, n-1) + 0, # Insert\n",
    "        self._editDistance(str1, str2, m-1, n) + 0, # Remove\n",
    "        self._editDistance(str1, str2, m-1, n-1)+ 1  # Replace\n",
    "        )\n",
    " \n",
    "  def predict_from_scratch(self,string, top_bests = 5):\n",
    "    # Keep Track of Top Best -> we need \"Fixed-size Max Heap\"\n",
    "    all = []\n",
    "    x = self._transliterate(string)\n",
    "    m = len(x)\n",
    "    for i in range(self.vocab_len):\n",
    "      y = self.vocab_eng[i]\n",
    "      n = len(y)\n",
    "      c = self._editDistance(x, y, m, n)\n",
    "      all.append( [c,self.vocab[i]] )\n",
    "\n",
    "    res = sorted(all, key = lambda x: x[0], reverse= False)\n",
    "    return res[:top_bests]\n",
    "\n",
    "  def _transliterate(self, string):\n",
    "    # encode persian to english (ASCII code)\n",
    "    res = ''\n",
    "    for s in string:\n",
    "      res += self.per_eng_dict[s]\n",
    "    return res\n",
    "\n",
    "  def fit(self, per_eng_dict,vocab_lst):\n",
    "    # Persian_to_English_dictionary is our decoding system\n",
    "    self.vocab = vocab_lst\n",
    "    self.vocab_len = len(vocab_lst)\n",
    "    self.per_eng_dict = per_eng_dict\n",
    "    # calculate correspoding transliteration for each token in vocab list\n",
    "    vocab_eng = []\n",
    "    for tok in self.vocab:\n",
    "      vocab_eng.append(self._transliterate(tok))\n",
    "    self.vocab_eng = vocab_eng\n",
    "\n",
    "\n",
    "  def predict(self,string, top_bests = 5):\n",
    "    # Keep Track of Top Best -> we need \"Fixed-size Max Heap\"\n",
    "    all = []\n",
    "    x = self._transliterate(string)\n",
    "    for i in range(self.vocab_len):\n",
    "      y = self.vocab_eng[i]\n",
    "      c = lev(x, y, insert_costs=self.in_costs,\n",
    "              delete_costs=self.dl_costs, substitute_costs=self.sb_costs)\n",
    "      all.append( [c,self.vocab[i]] )\n",
    "\n",
    "    res = sorted(all, key = lambda x: x[0], reverse= False)\n",
    "    return res[:top_bests]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "RPUCNGEuBPYx"
   },
   "outputs": [],
   "source": [
    "def accum_ngram(corp , n_gram = 5):\n",
    "    \n",
    "    # Python dictionary has dynamic hashing and can access each key's value \n",
    "    # in O(1) time. It, however, needs O(n) space to store n strings.  \n",
    "    # \"Trie\" is an attempt to reduce this space complexity\n",
    "    \n",
    "    # accumulated n-grams \n",
    "    # by accumulated I mean we compute all n-grams from 1 up to n\n",
    "    # Result will be stored and returned as a trie, implemented with dictionary\n",
    "    \n",
    "    n_grams = {}\n",
    "    n_grams['#'] = 0 \n",
    "    for stc in corp:\n",
    "        n = len(stc)\n",
    "        n_grams['#'] += n # update total words count\n",
    "        for token_idx in range(n):\n",
    "            dict_ptr = n_grams\n",
    "            for next_token_idx in range(token_idx,n):\n",
    "                if next_token_idx - token_idx >= n_gram:\n",
    "                    break\n",
    "                if not stc[next_token_idx] in dict_ptr:\n",
    "                    dict_ptr[stc[next_token_idx]] = {'#':0}\n",
    "                    \n",
    "                dict_ptr[stc[next_token_idx]]['#'] += 1\n",
    "                dict_ptr = dict_ptr[stc[next_token_idx]]\n",
    "                \n",
    "            \n",
    "    return n_grams\n",
    "\n",
    "\n",
    "# Here (in $$$ tagged line) I am using encouraged EAFP style\n",
    "# https://docs.python.org/3.6/glossary.html#term-eafp\n",
    "# https://stackoverflow.com/questions/1835756/using-try-vs-if-in-python\n",
    "\n",
    "def count_finder(sentence, n_grams_dict, j, i,mu,k):\n",
    "    # find both numerator and denominator \n",
    "    numerat ,denominat= k,mu\n",
    "    ptr_dict = n_grams_dict\n",
    "    flag = 1\n",
    "    for id in range(j,i):\n",
    "      # $$$\n",
    "      try: \n",
    "        ptr_dict = ptr_dict[sentence[id]]\n",
    "      except:\n",
    "        # if we didn't have a key = sentence[id] in our tire\n",
    "        # it means we don't have both numerator and denominator n_grams \n",
    "        flag = 0\n",
    "        break\n",
    "\n",
    "    if flag:\n",
    "      denominat += ptr_dict['#']\n",
    "\n",
    "    # Even if we have the n-gram associated with denominator\n",
    "    # chances are that we don't have n-gram associated with numerator\n",
    "    # since numerator's n-gram is longer that denominator's\n",
    "    # following try-except statement is to check this\n",
    "    try:\n",
    "      tmp = ptr_dict[sentence[i]]['#']\n",
    "      numerat += tmp\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "    return numerat ,denominat\n",
    "\n",
    "def occurance_drichlet_probability(sentence, n_grams_dict, n_gram, mu,Pbg):\n",
    "  # n_gram: numeber of previous words each conditional probability depends on\n",
    "  # sentence: list of tokens\n",
    "\n",
    "  # mu = k * v , Pbg= 1/v\n",
    "  k = mu* Pbg\n",
    "  \n",
    "  # logic for finding uni-grams is slightly different than other grams\n",
    "  # that's why we are handling uni-gram seperately\n",
    "  if n_gram ==1:\n",
    "      prob = 1\n",
    "      for token in sentence:\n",
    "          prob *= (n_grams_dict[token]['#']+k)\n",
    "      prob *= (1/((n_grams_dict['#']+mu)**len(sentence)))\n",
    "      return prob\n",
    "\n",
    "  # here we handle x-grams: x>=2    \n",
    "  # handeling the first fraction separately\n",
    "  # # put the following line into try except, later :) \n",
    "  # prob = (n_grams_dict[sentence[0]]['#']+k)/(n_grams_dict['#']+mu) \n",
    "  first_num = 0\n",
    "  first_deno = 0\n",
    "  try:\n",
    "    prob = (n_grams_dict[sentence[0]]['#']+k)/(n_grams_dict['#']+mu)\n",
    "  except:\n",
    "    prob = k/mu\n",
    "\n",
    "  \n",
    "  for i in range(1,len(sentence)):\n",
    "      j = i - (n_gram -1) if (i-n_gram>=0) else 0\n",
    "      numerator, denominator =count_finder(sentence,n_grams_dict,j,i,mu,k)\n",
    "      prob *= (numerator/denominator)\n",
    "      \n",
    "  return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "tG5LJfy3M95d"
   },
   "outputs": [],
   "source": [
    "def question6_handler(candidate_words,bigrams_sent,accum_bigrams, mu,Pbg):\n",
    "  i = -1\n",
    "  for sent in bigrams_sent:\n",
    "    i+=1\n",
    "    best_w, best_prob = \"\", 0\n",
    "    s = sent[1].split()\n",
    "\n",
    "    for cand in candidate_words[i]:\n",
    "      s[sent[0]] = cand\n",
    "      p = occurance_drichlet_probability(s, accum_bigrams,n_gram=2, \n",
    "                                         mu = mu, Pbg = Pbg)\n",
    "      if p > best_prob:\n",
    "        best_prob = p\n",
    "        best_w = cand\n",
    "    print(f'Based on Bigrams, best word:{best_w} with prob:{best_prob}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPkjtP97tu07"
   },
   "source": [
    "# Driver code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6hgZ27__21I"
   },
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "T3BKtIAotwWn"
   },
   "outputs": [],
   "source": [
    "sent = [\n",
    "        'انتخابات هفته آینده رادر تهران برگزار میکنیم ، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است',\n",
    "        'توانتشارخبرمهم در رسانه ها را تایید کردی',\n",
    "        'پسردکتراحمدی درکارنامه حرفه ای خود ۱۸ سریال تلویزیونی و ۱۲ فیلم سینمایی دارد',\n",
    "        'نقّاشی ساختمان به پایان رسیدددددددد',\n",
    "        'اللة اکبر گویان وارد مسجد شد',\n",
    "        'دکتر جان مممممممممممممممنونم که مشکلات حرکتي فرزندم را درمان کردید',\n",
    "]\n",
    "\n",
    "words = [\n",
    "        'اختصاد',\n",
    "        'سادرات',\n",
    "        'فوتکال',\n",
    "        'مسابغاط',\n",
    "        'وازدات',\n",
    "        'مشرکت',\n",
    "        'کشوور',\n",
    "        'منجلسه',\n",
    "]\n",
    "\n",
    "\n",
    "bigrams_sent = [\n",
    "                 (1 ,\"رشد اختصاد و تحرك زندگی اجتماعی\"),\n",
    "                 (1, 'حجم سادرات ایران'),\n",
    "                 (1, 'فدراسیون فوتکال کشور'),\n",
    "                 (3 ,'در جریان انعکاس مسابغاط صبح'),\n",
    "                 (2, 'اقلام عمده وازدات کشور'),\n",
    "                 (1 ,'اصل مشرکت مردمی'),\n",
    "                 (1, 'وزارت کشوور جمهوری اسلامی ایران'),\n",
    "                 (3 ,'جلسه علنی دیروز منجلسه شورای اسلامی'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZfv2aIU86ln"
   },
   "source": [
    "## Q1 (Norm, tok) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "Mk0VOFR8tjDV"
   },
   "outputs": [],
   "source": [
    "!gdown --id 19n8IMFUzjHFrDe7N-AkVHpyBUeuuXory\n",
    "!gdown --id 1Q1DZWyGv0HsNr4fwDxgBMl92P3Ntlq0B\n",
    "cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U15AZ1N-86Ia",
    "outputId": "e70fdebd-7099-4a51-c95f-d0c0f101626d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['انتخابات', 'هفته', 'آینده', 'رادر', 'تهران', 'برگزار', 'میکنیم', '،', 'اما', 'دسته', 'بندی', 'حوزهها', 'برای', 'رأی', 'گیری', 'از', 'این', 'هفته', 'آغاز', 'شده_است']\n",
      "انتخابات هفته آینده رادر تهران برگزار میکنیم، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است\n",
      "\n",
      "['توانتشارخبرمهم', 'در', 'رسانه', 'ها', 'را', 'تایید', 'کردی']\n",
      "توانتشارخبرمهم در رسانه‌ها را تایید کردی\n",
      "\n",
      "['پسردکتراحمدی', 'درکارنامه', 'حرفه', 'ای', 'خود', '۱۸', 'سریال', 'تلویزیونی', 'و', '۱۲', 'فیلم', 'سینمایی', 'دارد']\n",
      "پسردکتراحمدی درکارنامه حرفه‌ای خود ۱۸ سریال تلویزیونی و ۱۲ فیلم سینمایی دارد\n",
      "\n",
      "['نقّاشی', 'ساختمان', 'به', 'پایان', 'رسیدددددددد']\n",
      "نقاشی ساختمان به پایان رسیدددددددد\n",
      "\n",
      "['اللة', 'اکبر', 'گویان', 'وارد', 'مسجد', 'شد']\n",
      "اللة اکبر گویان وارد مسجد شد\n",
      "\n",
      "['دکتر', 'جان', 'مممممممممممممممنونم', 'که', 'مشکلات', 'حرکتي', 'فرزندم', 'را', 'درمان', 'کردید']\n",
      "دکتر جان مممممممممممممممنونم که مشکلات حرکتی فرزندم را درمان کردید\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  ----------------- Hazm ---------------------- \"\"\"\n",
    "hz_tokenizer = hazm.word_tokenize\n",
    "hz_normalizer = hazm.Normalizer().normalize\n",
    "for s in sent:\n",
    "  print(hz_tokenizer(s))\n",
    "  print(hz_normalizer(s))\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwRF2Qj-2Meq",
    "outputId": "2f431ee9-5580-46fb-bd4b-1eec97c7b5f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ----------------- Hazm ----------------------  \n",
      " Normaliz then tokenize\n",
      "\n",
      "\n",
      "انتخابات\n",
      "هفته\n",
      "آینده\n",
      "رادر\n",
      "تهران\n",
      "برگزار\n",
      "میکنیم\n",
      "،\n",
      "اما\n",
      "دسته\n",
      "بندی\n",
      "حوزهها\n",
      "برای\n",
      "رأی\n",
      "گیری\n",
      "از\n",
      "این\n",
      "هفته\n",
      "آغاز\n",
      "شده_است\n",
      "\n",
      "Next sentence\n",
      "توانتشارخبرمهم\n",
      "در\n",
      "رسانه‌ها\n",
      "را\n",
      "تایید\n",
      "کردی\n",
      "\n",
      "Next sentence\n",
      "پسردکتراحمدی\n",
      "درکارنامه\n",
      "حرفه‌ای\n",
      "خود\n",
      "۱۸\n",
      "سریال\n",
      "تلویزیونی\n",
      "و\n",
      "۱۲\n",
      "فیلم\n",
      "سینمایی\n",
      "دارد\n",
      "\n",
      "Next sentence\n",
      "نقاشی\n",
      "ساختمان\n",
      "به\n",
      "پایان\n",
      "رسیدددددددد\n",
      "\n",
      "Next sentence\n",
      "اللة\n",
      "اکبر\n",
      "گویان\n",
      "وارد\n",
      "مسجد\n",
      "شد\n",
      "\n",
      "Next sentence\n",
      "دکتر\n",
      "جان\n",
      "مممممممممممممممنونم\n",
      "که\n",
      "مشکلات\n",
      "حرکتی\n",
      "فرزندم\n",
      "را\n",
      "درمان\n",
      "کردید\n",
      "\n",
      "Next sentence\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"  ----------------- Hazm ---------------------- \"\"\",\n",
    "      '\\n','Normaliz then tokenize\\n\\n')\n",
    "hz_tokenizer = hazm.word_tokenize\n",
    "hz_normalizer = hazm.Normalizer().normalize\n",
    "for s in sent:\n",
    "  x = hz_tokenizer(hz_normalizer(s))\n",
    "  for tok in x:\n",
    "    print(tok)\n",
    "  print('\\nNext sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UaL41hSt9GrY",
    "outputId": "ea9241e2-5e7b-4a93-c3fc-4a29489ddcd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['انتخابات', 'هفته', 'آینده', 'رادر', 'تهران', 'برگزار', 'میکنیم', '،', 'اما', 'دسته', 'بندی', 'حوزهها', 'برای', 'رأی', 'گیری', 'از', 'این', 'هفته', 'آغاز', 'شده', 'است']\n",
      "انتخابات هفته آینده رادر تهران برگزار می‌کنیم ، اما دسته‌بندی حوزهها برای رای‌گیری از این هفته آغاز‌شده‌است\n",
      "\n",
      "['توانتشارخبرمهم', 'در', 'رسانه', 'ها', 'را', 'تایید', 'کردی']\n",
      "توانتشارخبرمهم در رسانه‌ها را تایید کردی\n",
      "\n",
      "['پسردکتراحمدی', 'درکارنامه', 'حرفه', 'ای', 'خود', '۱۸', 'سریال', 'تلویزیونی', 'و', '۱۲', 'فیلم', 'سینمایی', 'دارد']\n",
      "پسردکتراحمدی درکارنامه حرفه‌ای خود 18 سریال تلویزیونی و 12 فیلم سینمایی دارد\n",
      "\n",
      "['نقّاشی', 'ساختمان', 'به', 'پایان', 'رسیدددددددد']\n",
      "نقّاشی ساختمان به پایان رسیدددددددد\n",
      "\n",
      "['اللة', 'اکبر', 'گویان', 'وارد', 'مسجد', 'شد']\n",
      "الله اکبر گویان وارد مسجد شد\n",
      "\n",
      "['دکتر', 'جان', 'مممممممممممممممنونم', 'که', 'مشکلات', 'حرکتي', 'فرزندم', 'را', 'درمان', 'کردید']\n",
      "دکتر جان مممممممممممممممنونم که مشکلات حرکتی فرزندم را درمان کردید\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  ----------------- Parsivar ---------------------- \"\"\"\n",
    "ps_tokenizer = parsivar.Tokenizer().tokenize_words\n",
    "ps_normalizer = parsivar.Normalizer().normalize\n",
    "for s in sent:\n",
    "  print(ps_tokenizer(doc_string = s))\n",
    "  print(ps_normalizer(doc_string = s))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Juj1-g0T1Ju2",
    "outputId": "40f84ecd-91a6-459b-e216-ed2a9ff63667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ----------------- Parsivar ----------------------  \n",
      " Normaliz then tokenize\n",
      "\n",
      "\n",
      "انتخابات\n",
      "هفته\n",
      "آینده\n",
      "رادر\n",
      "تهران\n",
      "برگزار\n",
      "می‌کنیم\n",
      "،\n",
      "اما\n",
      "دسته‌بندی\n",
      "حوزهها\n",
      "برای\n",
      "رای‌گیری\n",
      "از\n",
      "این\n",
      "هفته\n",
      "آغاز‌شده‌است\n",
      "\n",
      "Next sentence\n",
      "توانتشارخبرمهم\n",
      "در\n",
      "رسانه‌ها\n",
      "را\n",
      "تایید\n",
      "کردی\n",
      "\n",
      "Next sentence\n",
      "پسردکتراحمدی\n",
      "درکارنامه\n",
      "حرفه‌ای\n",
      "خود\n",
      "18\n",
      "سریال\n",
      "تلویزیونی\n",
      "و\n",
      "12\n",
      "فیلم\n",
      "سینمایی\n",
      "دارد\n",
      "\n",
      "Next sentence\n",
      "نقّاشی\n",
      "ساختمان\n",
      "به\n",
      "پایان\n",
      "رسیدددددددد\n",
      "\n",
      "Next sentence\n",
      "الله\n",
      "اکبر\n",
      "گویان\n",
      "وارد\n",
      "مسجد\n",
      "شد\n",
      "\n",
      "Next sentence\n",
      "دکتر\n",
      "جان\n",
      "مممممممممممممممنونم\n",
      "که\n",
      "مشکلات\n",
      "حرکتی\n",
      "فرزندم\n",
      "را\n",
      "درمان\n",
      "کردید\n",
      "\n",
      "Next sentence\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"  ----------------- Parsivar ---------------------- \"\"\",\n",
    "      '\\n','Normaliz then tokenize\\n\\n')\n",
    "ps_tokenizer = parsivar.Tokenizer().tokenize_words\n",
    "ps_normalizer = parsivar.Normalizer().normalize\n",
    "for s in sent:\n",
    "  x = ps_tokenizer(ps_normalizer(doc_string = s))\n",
    "  for tok in x:\n",
    "    print(tok)\n",
    "  print('\\nNext sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "1gXvkEXUwYuT",
    "outputId": "52f2dc1f-b0ad-4972-f49f-74c5f475f78c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'              *** IMPORTANT NOTE ***: \\nI have run this cell on my local machine.\\nSince Colab cannot import PROTOCOL_TLS I used my own system.\\nPROTOCOL_TLS is available for py 3.8> but Colab uses py 3.7\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"  ----------------- Dadmatools ---------------------- \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"              *** IMPORTANT NOTE ***: \n",
    "I have run this cell on my local machine.\n",
    "Since Colab cannot import PROTOCOL_TLS I used my own system.\n",
    "PROTOCOL_TLS is available for py 3.8> but Colab uses py 3.7\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# !pip install dadmatools \n",
    "# import dadmatools\n",
    "\n",
    "# sent = [\n",
    "# 'انتخابات هفته آینده رادر تهران برگزار میکنیم ، اما دسته بندی حوزهها برای رأی گیری از این هفته آغاز شده است',\n",
    "# 'توانتشارخبرمهم در رسانه ها را تایید کردی',\n",
    "# 'پسردکتراحمدی درکارنامه حرفه ای خود ۱۸ سریال تلویزیونی و ۱۲ فیلم سینمایی دارد',\n",
    "# 'نقّاشی ساختمان به پایان رسیدددددددد',\n",
    "# 'اللة اکبر گویان وارد مسجد شد',\n",
    "# 'دکتر جان مممممممممممممممنونم که مشکلات حرکتي فرزندم را درمان کردید',\n",
    "# ]\n",
    "\n",
    "# from dadmatools.models.normalizer import Normalizer\n",
    "\n",
    "# normalizer = Normalizer(\n",
    "#     full_cleaning=False,\n",
    "#     unify_chars=True,\n",
    "#     refine_punc_spacing=True,\n",
    "#     remove_extra_space=True,\n",
    "#     remove_puncs=False,\n",
    "#     remove_html=False,\n",
    "#     remove_stop_word=False,\n",
    "#     replace_email_with=\"<EMAIL>\",\n",
    "#     replace_number_with=None,\n",
    "#     replace_url_with=\"\",\n",
    "#     replace_mobile_number_with=None,\n",
    "#     replace_emoji_with=None,\n",
    "#     replace_home_number_with=None\n",
    "# )\n",
    "\n",
    "# dm_normalized_lst = []\n",
    "# for s in sent:\n",
    "#     dm_normalized_lst.append(normalizer.normalize(s))\n",
    "# # dm_normalized_lst\n",
    "\n",
    "# from  dadmatools.pipeline import language \n",
    "# # here lemmatizer and pos tagger will be loaded\n",
    "# # as tokenizer is the default tool, it will be loaded as well even without calling\n",
    "# pips = 'tok,lem' \n",
    "# nlp = language.Pipeline(pips)\n",
    "\n",
    "# # you can see the pipeline with this code\n",
    "# print(nlp.analyze_pipes(pretty=True))\n",
    "\n",
    "\n",
    "# dm_final_result = []\n",
    "# for d in dm_docs:\n",
    "#     dm_final_result.append(language.to_json(pips, d))\n",
    "\n",
    "# dm_final_result\n",
    "\n",
    "# import pickle\n",
    "# with open('dm_normalized_lst.pkl', 'wb') as f:\n",
    "#     pickle.dump(dm_normalized_lst, f)\n",
    "\n",
    "# import pickle\n",
    "# with open('dm_final_result.pkl', 'wb') as f:\n",
    "#     pickle.dump(dm_final_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLgl42UDeSc2",
    "outputId": "6de8021f-2076-41c5-8496-96c1a02bb3b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['انتخابات هفته آینده رادر تهران برگزار میکنیم، اما دسته بندی حوزهها برای رای گیری از این هفته آغاز شده است',\n",
       " 'توانتشارخبرمهم در رسانه ها را تایید کردی',\n",
       " 'پسردکتراحمدی درکارنامه حرفه ای خود 18 سریال تلویزیونی و 12 فیلم سینمایی دارد',\n",
       " 'نقّاشی ساختمان به پایان رسیدددددددد',\n",
       " 'الله اکبر گویان وارد مسجد شد',\n",
       " 'دکتر جان مممممممممممممممنونم که مشکلات حرکتی فرزندم را درمان کردید']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('dm_normalized_lst.pkl', 'rb') as f:\n",
    "  dm_normalized = pickle.load(f)\n",
    "\n",
    "dm_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4a8kxm70xxM"
   },
   "source": [
    "### Part one Corrolary: \n",
    "1) Case 1: \n",
    "\"\"\" آغاز شده است\"\"\"\n",
    "\n",
    "Hazm: شده_است\n",
    "\n",
    "parsivar: آغازشده‌است\n",
    "\n",
    "dadma: آغاز شده است\n",
    "\n",
    "Unexpectedly, Parsivar has outperformed other libraries in normalizing compound verbs in persian.\n",
    "\n",
    "2) Case 2:  \" رای \"\n",
    "\n",
    "both Parsivar and dadma have done a pretty good job in removing \"ء\". Yet, all libraries have a weakness in this task. For instance non of them has removed \" Tashdid\" from \" نقّاشی \" \n",
    "\n",
    "3) Case 3 : \"مممممممنونم\"\n",
    "\n",
    "Needless to say, all of them have failed to normalize this case and many others.\n",
    "\n",
    "3) Case 3 : \"رادر\"\n",
    "\n",
    "Needless to say, all of them have failed to tokenize this case and many others.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjwXgiir9HFJ"
   },
   "source": [
    "## Q2 (Stemming/Lemma, StopWords Removal)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N25NuAQj9RkB",
    "outputId": "310c1525-458d-4818-b5ec-662cd43036fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of stopwrods:['اما', 'دسته', 'بندی', 'برای', 'گیری', 'از', 'این', 'شده', 'است']\n",
      "stemming: ['انتخاب', 'هفته', 'آینده', 'رادر', 'تهر', 'برگزار', 'میکن', '،', 'اما', 'دسته', 'بند', 'حوزه', 'برا', 'رأ', 'گیر', 'از', 'این', 'هفته', 'آغاز', 'شده', 'اس']\n",
      "lemming: ['انتخابات', 'هفته', 'آینده', 'رادر', 'تهران', 'برگزار', 'میکنیم', '،', 'اما', 'دسته', 'بست#بند', 'حوزه', 'برای', 'رأی', 'گرفت#گیر', 'از', 'این', 'هفته', 'آغاز', 'شده', '#است']\n",
      "\n",
      "\n",
      "\n",
      "List of stopwrods:['در', 'را']\n",
      "stemming: ['توانتشارخبرمه', 'در', 'رسانه', '', 'را', 'تایید', 'کرد']\n",
      "lemming: ['توانتشارخبرمهم', 'در', 'رسانه', 'ها', 'را', 'تایید', 'کردی']\n",
      "\n",
      "\n",
      "\n",
      "List of stopwrods:['خود', 'و', 'دارد']\n",
      "stemming: ['پسردکتراحمد', 'درکارنامه', 'حرفه', 'ا', 'خود', '۱۸', 'سریال', 'تلویزیون', 'و', '۱۲', 'فیل', 'سینما', 'دارد']\n",
      "lemming: ['پسردکتراحمدی', 'درکارنامه', 'حرفه', 'ای', 'خود', '۱۸', 'سریال', 'تلویزیون', 'و', '۱۲', 'فیلم', 'سینمایی', 'داشت#دار']\n",
      "\n",
      "\n",
      "\n",
      "List of stopwrods:['به']\n",
      "stemming: ['نقّاش', 'ساخ', 'به', 'پا', 'رسیدددددددد']\n",
      "lemming: ['نقّاش', 'ساختمان', 'به', 'پایان', 'رسیدددددددد']\n",
      "\n",
      "\n",
      "\n",
      "List of stopwrods:['وارد', 'شد']\n",
      "stemming: ['اللة', 'اکبر', 'گو', 'وارد', 'مسجد', 'شد']\n",
      "lemming: ['اللة', 'اکبر', 'گویان', 'وارد', 'مسجد', 'شد#شو']\n",
      "\n",
      "\n",
      "\n",
      "List of stopwrods:['که', 'را']\n",
      "stemming: ['دک', 'ج', 'مممممممممممممممنون', 'که', 'مشکل', 'حرکتي', 'فرزند', 'را', 'در', 'کردید']\n",
      "lemming: ['دکتر', 'جان', 'مممممممممممممممنونم', 'که', 'مشکلات', 'حرکتي', 'فرزند', 'را', 'درمان', 'کرد#کن']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  ----------------- Hazm ---------------------- \"\"\"\n",
    "hz_stops = hazm.stopwords_list()\n",
    "hz_stemmer  = hazm.Stemmer().stem\n",
    "hz_lemmatizer = hazm.Lemmatizer().lemmatize\n",
    "\n",
    "for s in sent:\n",
    "  toks = s.strip().split()\n",
    "  stems = [hz_stemmer(t) for t in toks]\n",
    "  lems = [hz_lemmatizer(t) for t in toks]\n",
    "  stp = [tok for tok in toks if tok in hz_stops ]\n",
    "  print(f'List of stopwrods:{stp}')\n",
    "  print(f'stemming: {stems}')\n",
    "  print(f'lemming: {lems}')\n",
    "  print('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "Y-MzFint9f8o",
    "outputId": "19c2c65c-505e-4939-bc8b-87c8e62fcad6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nfunctionalities:\\n          Text Normalizing\\n          Half space correction in Persian text\\n          Word and sentence tokenizer (splitting words and sentences)\\n          Word stemming\\n          POS tagger\\n          Shallow parser (Chunker)\\n          Dependency Parser\\n          Spell Checker\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"  ----------------- Parsivar ---------------------- \"\"\"\n",
    "\n",
    "''' Note1: Parsivar only support following functionalities '''\n",
    "''' Note2: Parsivar's Stemmer Doesn't have any interface'''\n",
    "\n",
    "\"\"\"\n",
    "functionalities:\n",
    "          Text Normalizing\n",
    "          Half space correction in Persian text\n",
    "          Word and sentence tokenizer (splitting words and sentences)\n",
    "          Word stemming\n",
    "          POS tagger\n",
    "          Shallow parser (Chunker)\n",
    "          Dependency Parser\n",
    "          Spell Checker\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "nbpMOyzPFn-C",
    "outputId": "80a043a4-07c0-4a59-9136-d6ec69156a39"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Note: Dadmatools Doesn't remove stopword, only lemmetization\\nYou can see its lemmetization result in the next part\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"  ----------------- Dadmatools ---------------------- \"\"\"\n",
    "'''Note: Dadmatools Doesn't remove stopword, only lemmetization\n",
    "You can see its lemmetization result in the next part'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdElZIhL5tZX"
   },
   "source": [
    "### Part two Corrolary: \n",
    "1) Case 1: \n",
    "\"\"\" فیلم >>>‌  فیل\"\"\n",
    "\n",
    "\n",
    "Hazm: hazm stemmes \" Film \" into \" Fil \" that might cuase ambiguity in some specific tasks, like categorization.\n",
    "\n",
    "\n",
    "More than what I have already described, there is no discernible effect or behaviour.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "5v0AV-dk5st7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQIgSSSZ9gbm"
   },
   "source": [
    "## Q3 (Question 2 and 3 consecutively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86uPF2pgFooE",
    "outputId": "8945cf16-95bb-4b6f-8649-807f66adb9d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of stopwrods:['اما', 'دسته', 'بندی', 'برای', 'گیری', 'از', 'این', 'شده_است']\n",
      "Out Put of HAZM pipeline after stemming : ['انتخاب', 'هفته', 'آینده', 'رادر', 'تهر', 'برگزار', 'میکن', '،', 'اما', 'دسته', 'بند', 'حوزه', 'برا', 'رأ', 'گیر', 'از', 'این', 'هفته', 'آغاز', 'شده_اس']\n",
      "Out Put of HAZM pipeline after lemming  : ['انتخابات', 'هفته', 'آینده', 'رادر', 'تهران', 'برگزار', 'میکنیم', '،', 'اما', 'دسته', 'بست#بند', 'حوزه', 'برای', 'رأی', 'گرفت#گیر', 'از', 'این', 'هفته', 'آغاز', 'شد#شو']\n",
      "\n",
      "\n",
      "\n",
      "List of stopwrods:['در', 'را']\n",
      "Out Put of HAZM pipeline after stemming : ['توانتشارخبرمه', 'در', 'رسانه', 'را', 'تایید', 'کرد']\n",
      "Out Put of HAZM pipeline after lemming  : ['توانتشارخبرمهم', 'در', 'رسانه', 'را', 'تایید', 'کردی']\n",
      "\n",
      "\n",
      "\n",
      "List of stopwrods:['خود', 'و', 'دارد']\n",
      "Out Put of HAZM pipeline after stemming : ['پسردکتراحمد', 'درکارنامه', 'حرفه', 'خود', '۱۸', 'سریال', 'تلویزیون', 'و', '۱۲', 'فیل', 'سینما', 'دارد']\n",
      "Out Put of HAZM pipeline after lemming  : ['پسردکتراحمدی', 'درکارنامه', 'حرفه\\u200cای', 'خود', '۱۸', 'سریال', 'تلویزیون', 'و', '۱۲', 'فیلم', 'سینمایی', 'داشت#دار']\n",
      "\n",
      "\n",
      "\n",
      "List of stopwrods:['به']\n",
      "Out Put of HAZM pipeline after stemming : ['نقاش', 'ساخ', 'به', 'پا', 'رسیدددددددد']\n",
      "Out Put of HAZM pipeline after lemming  : ['نقاش', 'ساختمان', 'به', 'پایان', 'رسیدددددددد']\n",
      "\n",
      "\n",
      "\n",
      "List of stopwrods:['وارد', 'شد']\n",
      "Out Put of HAZM pipeline after stemming : ['اللة', 'اکبر', 'گو', 'وارد', 'مسجد', 'شد']\n",
      "Out Put of HAZM pipeline after lemming  : ['اللة', 'اکبر', 'گویان', 'وارد', 'مسجد', 'شد#شو']\n",
      "\n",
      "\n",
      "\n",
      "List of stopwrods:['که', 'را']\n",
      "Out Put of HAZM pipeline after stemming : ['دک', 'ج', 'مممممممممممممممنون', 'که', 'مشکل', 'حرکت', 'فرزند', 'را', 'در', 'کردید']\n",
      "Out Put of HAZM pipeline after lemming  : ['دکتر', 'جان', 'مممممممممممممممنونم', 'که', 'مشکلات', 'حرکت', 'فرزند', 'را', 'درمان', 'کرد#کن']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  ----------------- Hazm ---------------------- \"\"\"\n",
    "# hz_tokenizer = hazm.word_tokenize\n",
    "# hz_normalizer = hazm.Normalizer().normalize\n",
    "# for s in sent:\n",
    "#   print(hz_tokenizer(s))\n",
    "#   print(hz_normalizer(s))\n",
    "#   print()\n",
    "# hz_stops = hazm.stopwords_list()\n",
    "# hz_stemmer  = hazm.Stemmer().stem\n",
    "# hz_lemmatizer = hazm.Lemmatizer().lemmatize\n",
    "\n",
    "for s in sent:\n",
    "  s = hz_normalizer(s)\n",
    "  toks = hz_tokenizer(s)\n",
    "  stems = [hz_stemmer(t) for t in toks]\n",
    "  lems = [hz_lemmatizer(t) for t in toks]\n",
    "  stp = [tok for tok in toks if tok in hz_stops ]\n",
    "  print(f'List of stopwrods:{stp}')\n",
    "  print(f'Out Put of HAZM pipeline after stemming : {stems}')\n",
    "  print(f'Out Put of HAZM pipeline after lemming  : {lems}')\n",
    "  print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pWB9WKvDFojK",
    "outputId": "6b51e6e3-3e9a-4006-bea5-b1af0d8e5dc2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Not applicable '"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"  ----------------- Parsivar ---------------------- \"\"\"\n",
    "''' Not applicable '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gY9cCNZ9qI9",
    "outputId": "ab03fbe4-d8e9-4042-f130-a84a684bf07a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[{'id': 1, 'lemma': 'انتخابات', 'text': 'انتخابات'},\n",
       "   {'id': 2, 'lemma': 'هفته', 'text': 'هفته'},\n",
       "   {'id': 3, 'lemma': 'آینده', 'text': 'آینده'},\n",
       "   {'id': 4, 'lemma': 'رادر', 'text': 'رادر'},\n",
       "   {'id': 5, 'lemma': 'تهران', 'text': 'تهران'},\n",
       "   {'id': 6, 'lemma': 'برگزار', 'text': 'برگزار'},\n",
       "   {'id': 7, 'lemma': 'میکنیم', 'text': 'میکنیم'},\n",
       "   {'id': 8, 'lemma': '،', 'text': '،'},\n",
       "   {'id': 9, 'lemma': 'اما', 'text': 'اما'},\n",
       "   {'id': 10, 'lemma': 'دسته', 'text': 'دسته'},\n",
       "   {'id': 11, 'lemma': 'بند', 'text': 'بندی'},\n",
       "   {'id': 12, 'lemma': 'حوزه', 'text': 'حوزهها'},\n",
       "   {'id': 13, 'lemma': 'برای', 'text': 'برای'},\n",
       "   {'id': 14, 'lemma': 'رای', 'text': 'رای'},\n",
       "   {'id': 15, 'lemma': 'گیر', 'text': 'گیری'},\n",
       "   {'id': 16, 'lemma': 'از', 'text': 'از'},\n",
       "   {'id': 17, 'lemma': 'این', 'text': 'این'},\n",
       "   {'id': 18, 'lemma': 'هفته', 'text': 'هفته'},\n",
       "   {'id': 19, 'lemma': 'آغاز', 'text': 'آغاز'},\n",
       "   {'id': 20, 'lemma': 'شد#شو', 'text': 'شده'},\n",
       "   {'id': 21, 'lemma': '#است', 'text': 'است'}]],\n",
       " [[{'id': 1, 'lemma': 'توانتشارخبرمهم', 'text': 'توانتشارخبرمهم'},\n",
       "   {'id': 2, 'lemma': 'در', 'text': 'در'},\n",
       "   {'id': 3, 'lemma': 'رسانه', 'text': 'رسانه'},\n",
       "   {'id': 4, 'lemma': 'ها', 'text': 'ها'},\n",
       "   {'id': 5, 'lemma': 'را', 'text': 'را'},\n",
       "   {'id': 6, 'lemma': 'تایید', 'text': 'تایید'},\n",
       "   {'id': 7, 'lemma': 'کرد#کن', 'text': 'کردی'}]],\n",
       " [[{'id': 1, 'lemma': 'پسردکتراحمدی', 'text': 'پسردکتراحمدی'},\n",
       "   {'id': 2, 'lemma': 'درکارنامه', 'text': 'درکارنامه'},\n",
       "   {'id': 3, 'lemma': 'حرفه', 'text': 'حرفه'},\n",
       "   {'id': 4, 'lemma': 'ای', 'text': 'ای'},\n",
       "   {'id': 5, 'lemma': 'خود', 'text': 'خود'},\n",
       "   {'id': 6, 'lemma': '18', 'text': '18'},\n",
       "   {'id': 7, 'lemma': 'سریال', 'text': 'سریال'},\n",
       "   {'id': 8, 'lemma': 'تلویزیونی', 'text': 'تلویزیونی'},\n",
       "   {'id': 9, 'lemma': 'و', 'text': 'و'},\n",
       "   {'id': 10, 'lemma': '12', 'text': '12'},\n",
       "   {'id': 11, 'lemma': 'فیلم', 'text': 'فیلم'},\n",
       "   {'id': 12, 'lemma': 'سینمایی', 'text': 'سینمایی'},\n",
       "   {'id': 13, 'lemma': 'داشت#دار', 'text': 'دارد'}]],\n",
       " [[{'id': 1, 'lemma': 'نقّاش', 'text': 'نقّاشی'},\n",
       "   {'id': 2, 'lemma': 'ساختمان', 'text': 'ساختمان'},\n",
       "   {'id': 3, 'lemma': 'به', 'text': 'به'},\n",
       "   {'id': 4, 'lemma': 'پایان', 'text': 'پایان'},\n",
       "   {'id': 5, 'lemma': 'رسیدددددددد', 'text': 'رسیدددددددد'}]],\n",
       " [[{'id': 1, 'lemma': 'الله', 'text': 'الله'},\n",
       "   {'id': 2, 'lemma': 'اکبر', 'text': 'اکبر'},\n",
       "   {'id': 3, 'lemma': 'گویان', 'text': 'گویان'},\n",
       "   {'id': 4, 'lemma': 'وارد', 'text': 'وارد'},\n",
       "   {'id': 5, 'lemma': 'مسجد', 'text': 'مسجد'},\n",
       "   {'id': 6, 'lemma': 'شد#شو', 'text': 'شد'}]],\n",
       " [[{'id': 1, 'lemma': 'دکتر', 'text': 'دکتر'},\n",
       "   {'id': 2, 'lemma': 'جان', 'text': 'جان'},\n",
       "   {'id': 3, 'lemma': 'مممممممممممممم', 'text': 'مممممممممممممم'},\n",
       "   {'id': 4, 'lemma': 'من', 'text': 'م'},\n",
       "   {'id': 5, 'lemma': 'که', 'text': 'که'},\n",
       "   {'id': 6, 'lemma': 'مشکل', 'text': 'مشکلات'},\n",
       "   {'id': 7, 'lemma': 'حرکت', 'text': 'حرکتی'},\n",
       "   {'id': 8, 'lemma': 'فرزند', 'text': 'فرزند'},\n",
       "   {'id': 9, 'lemma': 'من', 'text': 'م'},\n",
       "   {'id': 10, 'lemma': 'را', 'text': 'را'},\n",
       "   {'id': 11, 'lemma': 'درمان', 'text': 'درمان'},\n",
       "   {'id': 12, 'lemma': 'کرد#کن', 'text': 'کردید'}]]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"  ----------------- Dadmatools ---------------------- \"\"\"\n",
    "import pickle\n",
    "with open('dm_final_result.pkl', 'rb') as f:\n",
    "  dm_final_res = pickle.load(f)\n",
    "  \n",
    "dm_final_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLDC2O738x3w"
   },
   "source": [
    "### Part three Corrolary: \n",
    "\n",
    "1) Case 1: dadma seems to do the same job in lemmetization as Hazm. Nothing more.\n",
    "\n",
    "\n",
    "2) Case 2:  \" شده است \"\n",
    "\n",
    "Lemmitization on init text results in: 'شده', '#است\n",
    "\n",
    "However, Lemmitization on nomalized text results in \"شد#شو\"\n",
    "\n",
    "\n",
    "\n",
    "3) Case 3) BE ADVISED! IF YOU STEMM THEN REMOVE STOP WORDS, YOUR PREPROCESSING RESULT IN A BIASED TEXT.\n",
    "\n",
    "FOR INSTANCE, \"در\" is a stop word. درمان will be stemmed into 'در' by hazm lib. So if you do stemming and then stop word removal, you remove some important which may have discrimination power!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "CUfxq7pR9gyt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QS9A9RAv9qsO"
   },
   "source": [
    "## Q4 (Libraries Comparision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOe3IsdwnoRv"
   },
   "source": [
    "\n",
    "\n",
    "1.   Hazm: \n",
    "      \n",
    "     hazm supports wide range of functions for preprocessing.\n",
    "     Since it mostly uses Regular Expression for its preprocessing does not perform well in Normalization. Lastly, Hazm performs fine on lemmetization of verbs.  \n",
    "2.   Parsivar:\n",
    "\n",
    " Parsivar has a limited range of preprocessing funcitons. Does not have stop word list. Its stemming module does not have an interface and if you wanna use its stemming module, you gotta write an interface for it. like hazm, parsivar didn't normalize \"مممممنون\" or \"پسردکتراحمدی\" or others. \n",
    "\n",
    "3.   Dadmatools: unlike hazm and Parsivar, dadma uses new approaches like neural networks and transformers to reach higher performance on Persian NLP tasks. It, however, does not use new approaches for preprocessing (normalizing , tokenization, and lemmitization). That's why dadma does not outperform hazm on prerprocesing task.\n",
    "\n",
    "All in all, I prefer using Hazm for preprocessing tasks and Dadam for other Persian NLP tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_L--OJnqr5-"
   },
   "source": [
    "### Persian NLP Datasets That Dadmatools supports\n",
    "\n",
    "We provide an easy-to-use way to load some popular Persian NLP datasets\n",
    "Here is the list of supported datasets.\n",
    "\n",
    "   |    Dataset             | Task \n",
    "|       :----------------:               |  :----------------:   \n",
    "   |    PersianNER           |   Named Entity Recognition   | \n",
    "   |       ARMAN             |   Named Entity Recognition\n",
    "   |       Peyma             | Named Entity Recognition\n",
    "  |       FarsTail           | Textual Entailment\n",
    " |        FaSpell           | Spell Checking\n",
    "  |      PersianNews        | Text Classification\n",
    "  |       PerUDT            | Universal Dependency\n",
    "  |      PnSummary          | Text Summarization\n",
    "  |    SnappfoodSentiment   | Sentiment Classification\n",
    "  |           TEP           | Text Translation(eng-fa)\n",
    "| WikipediaCorpus               | Corpus\n",
    "| PersianTweets           | Corpus\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Evaluation Dadma VS Hamz\n",
    "Dadmatools have compared their pos tagging, dependancy parsing, and lemmatization models to `stanza` and `hazm`.\n",
    "\n",
    "<table>\n",
    "  <tr align='center'>\n",
    "    <td colspan=\"4\"><b>PerDT (F1 score)</b></td>\n",
    "  </tr>\n",
    "  <tr align='center'>\n",
    "    <td><b>Toolkit</b></td>\n",
    "    <td><b>POS Tagger (UPOS)</b></td>\n",
    "    <td><b>Dependancy Parser (UAS/LAS)</b></td>\n",
    "    <td><b>Lemmatizer</b></td>\n",
    "  </tr>\n",
    "  <tr align='center'>\n",
    "    <td>DadmaTools</td>\n",
    "    <td><b>97.52%</b></td>\n",
    "    <td><b>95.36%</b>  /  <b>92.54%</b> </td>\n",
    "    <td><b>99.14%</b> </td>\n",
    "  </tr>\n",
    "  <tr align='center'>\n",
    "    <td>stanza</td>\n",
    "    <td>97.35%</td>\n",
    "    <td>93.34%  /  91.05% </td>\n",
    "    <td>98.97% </td>\n",
    "  </tr>\n",
    "  <tr align='center'>\n",
    "    <td>hazm</td>\n",
    "    <td>-</td>\n",
    "    <td>- </td>\n",
    "    <td>89.01% </td>\n",
    "  </tr>\n",
    "\n",
    "\n",
    "  <tr align='center'>\n",
    "    <td colspan=\"4\"><b>Seraji (F1 score)</b></td>\n",
    "  </tr>\n",
    "  <tr align='center'>\n",
    "    <td><b>Toolkit</b></td>\n",
    "    <td><b>POS Tagger (UPOS)</b></td>\n",
    "    <td><b>Dependancy Parser (UAS/LAS)</b></td>\n",
    "    <td><b>Lemmatizer</b></td>\n",
    "  </tr>\n",
    "  <tr align='center'>\n",
    "    <td>DadmaTools</td>\n",
    "    <td><b>97.83%</b></td>\n",
    "    <td><b>92.5%</b>  /  <b>89.23%</b> </td>\n",
    "    <td> - </td>\n",
    "  </tr>\n",
    "  <tr align='center'>\n",
    "    <td>stanza</td>\n",
    "    <td>97.43%</td>\n",
    "    <td>87.20% /  83.89% </td>\n",
    "    <td> - </td>\n",
    "  </tr>\n",
    "  <tr align='center'>\n",
    "    <td>hazm</td>\n",
    "    <td>-</td>\n",
    "    <td>- </td>\n",
    "    <td>86.93% </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr align='center'>\n",
    "    <td colspan=\"2\"><b>Tehran university tree bank (F1 score)</b></td>\n",
    "  </tr>\n",
    "  <tr align='center'>\n",
    "    <td><b>Toolkit</b></td>\n",
    "    <td><b>Constituency Parser</b></td>\n",
    "  </tr>\n",
    "  <tr align='center'>\n",
    "    <td>DadmaTools (without preprocess))</td>\n",
    "    <td><b>82.88%</b></td>\n",
    "  </tr>\n",
    "  <tr align='center'>\n",
    "    <td>Stanford (with some preprocess on POS tags)</td>\n",
    "    <td>80.28</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUN7-r2prRTL"
   },
   "source": [
    "Reference: Dadmatools github\n",
    "https://github.com/Dadmatech/DadmaTools/edit/main/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LMMPiM_97qD"
   },
   "source": [
    "## Q5 (levenshtein distance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "fDT0qblj-QKu"
   },
   "outputs": [],
   "source": [
    "!gdown --id 1rovazK48q7pHcEM271aX70Dr594NYQ77\n",
    "!gdown --id 1ZCHuj6JtyOkb5ismn3qF3Rp2DRGJ1tRk\n",
    "cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBHlYmt9CRC1",
    "outputId": "73c08bc2-4e77-4c73-dd47-297bef398b99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['فری', 'توپ', 'درجا', 'قوام', 'کاتای', 'هماهنگ', 'غضروف', 'فلوریدای', 'قدمتی', 'توجیهی', 'جلوه', 'ادامه', 'اصالت', 'اتفاقیه', 'المانهای', 'کشکان', 'غشاأ', 'تهرانپارس', 'فرضیات', 'بازنگری', 'ملکوت', 'جواد', 'حالت', 'کلیث', 'موقتی', 'تار', 'عادات', 'شیای', 'تعبیرات', 'واحدی']\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "tmp1 = train_df['article'].tolist() \n",
    "all = tmp1 + test_df['article'].tolist()\n",
    "\n",
    "parsi = FarsiParsi()\n",
    "all_cooked = [parsi.clean_tokenize(x) for x in all]\n",
    "d = {}\n",
    "for lst in all_cooked:\n",
    "  for tok in lst:\n",
    "    if tok in d:\n",
    "        d[tok] += 1\n",
    "    else:\n",
    "        d[tok] = 1\n",
    "vocab_lst = []\n",
    "for k,v in d.items():\n",
    "  if v>3:\n",
    "    vocab_lst.append(k)\n",
    "\n",
    "from random import sample\n",
    "print(sample(vocab_lst,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "4JlwDu0ULG2o"
   },
   "outputs": [],
   "source": [
    "chars = {}\n",
    "for tok in vocab_lst:\n",
    "  for char in tok:\n",
    "    if not char in chars:\n",
    "        chars[char] = 1\n",
    "\n",
    "\"\"\"\n",
    "More info about ASCII codes:\n",
    "https://miro.medium.com/max/1200/1*DdgD00dAdXggzMdWDt7GSA.png\n",
    "\"\"\"\n",
    "x = [a for a in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLM'] \n",
    "\n",
    "per_eng_dict = {}\n",
    "i = -1\n",
    "for k in  chars.keys():\n",
    "  i+=1 \n",
    "  per_eng_dict[k] = x[i]\n",
    "\n",
    "# >> per_eng_dict\n",
    "# {'_': 'G',\n",
    "#  'آ': 'C',\n",
    "#  'أ': 'J',\n",
    "#  'ؤ': 'H',\n",
    "#  'إ': 'K',\n",
    "#  'ا': 'e',\n",
    "#  'ب': 'n',\n",
    "#  'ت': 'm',\n",
    "#  'ث': 'A',\n",
    "#  'ج': 'u',\n",
    "#  'ح': 'o',\n",
    "#  'خ': 'v',\n",
    "#  . . ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szVySghLPCIX"
   },
   "source": [
    "### Weighted MINIMUM EDIT DISTANCE WITH LIB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SzImLoGOEpB",
    "outputId": "6fb3dce0-4cd0-45f4-f7dd-f6303b0c0357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on Min Edit Distance, bests for اختصاد are:\n",
      "[[2.0, 'اختصاص'], [2.0, 'اقتصاد'], [2.0, 'اختصار'], [3.0, 'اقتصادی'], [3.0, 'افتاد']] \n",
      "\n",
      "Based on Min Edit Distance, bests for سادرات are:\n",
      "[[1.0, 'سادات'], [2.0, 'خسارات'], [2.0, 'صادرات'], [2.0, 'سارا'], [2.0, 'ادات']] \n",
      "\n",
      "Based on Min Edit Distance, bests for فوتکال are:\n",
      "[[2.0, 'فوتبال'], [2.0, 'فوتسال'], [3.0, 'فوت'], [3.0, 'فواصل'], [3.0, 'تکامل']] \n",
      "\n",
      "Based on Min Edit Distance, bests for مسابغاط are:\n",
      "[[3.0, 'سابا'], [4.0, 'مساال'], [4.0, 'مسابقات'], [4.0, 'محابا'], [4.0, 'سابقا']] \n",
      "\n",
      "Based on Min Edit Distance, bests for وازدات are:\n",
      "[[2.0, 'واداشت'], [2.0, 'موازات'], [2.0, 'واردات'], [2.0, 'ادات'], [3.0, 'وزارت']] \n",
      "\n",
      "Based on Min Edit Distance, bests for مشرکت are:\n",
      "[[1.0, 'شرکت'], [1.0, 'مشارکت'], [1.0, 'مشرک'], [2.0, 'مشترک'], [2.0, 'شرکتی']] \n",
      "\n",
      "Based on Min Edit Distance, bests for کشوور are:\n",
      "[[1.0, 'کشور'], [2.0, 'شور'], [2.0, 'کشوری'], [2.0, 'کشورش'], [2.0, 'کور']] \n",
      "\n",
      "Based on Min Edit Distance, bests for منجلسه are:\n",
      "[[2.0, 'مجلس'], [2.0, 'جلسه'], [2.0, 'مجله'], [2.0, 'منجمله'], [3.0, 'مجسمه']] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['اختصاص', 'اقتصاد', 'اختصار', 'اقتصادی', 'افتاد'],\n",
       " ['سادات', 'خسارات', 'صادرات', 'سارا', 'ادات'],\n",
       " ['فوتبال', 'فوتسال', 'فوت', 'فواصل', 'تکامل'],\n",
       " ['سابا', 'مساال', 'مسابقات', 'محابا', 'سابقا'],\n",
       " ['واداشت', 'موازات', 'واردات', 'ادات', 'وزارت'],\n",
       " ['شرکت', 'مشارکت', 'مشرک', 'مشترک', 'شرکتی'],\n",
       " ['کشور', 'شور', 'کشوری', 'کشورش', 'کور'],\n",
       " ['مجلس', 'جلسه', 'مجله', 'منجمله', 'مجسمه']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell = SpellCorrection()\n",
    "spell.fit(per_eng_dict,vocab_lst)\n",
    "correction_candidates = []\n",
    "for w in words:\n",
    "  print(f'Based on Min Edit Distance, bests for {w} are:')\n",
    "  tmp = spell.predict(string = w , top_bests = 5 )\n",
    "  correction_candidates.append(tmp)\n",
    "  print(tmp,'\\n')\n",
    "\n",
    "candidate_words = []\n",
    "for sett in correction_candidates:\n",
    "  tmp = []\n",
    "  for can in sett:\n",
    "    tmp.append(can[1])\n",
    "  candidate_words.append(tmp)\n",
    "candidate_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69HhnANDPNoB"
   },
   "source": [
    "### Weighted MINIMUM EDIT DISTANCE FROM SCRACH ( WITHOUT ANY LIB )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Leopk0YUPZVJ",
    "outputId": "77708927-175f-4343-c8da-927997a4c1c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Scratch!, bests for اختصاد are:\n",
      "[[2, 'اختصاص'], [2, 'اقتصاد'], [2, 'اختصار'], [3, 'اقتصادی'], [3, 'افتاد']] \n",
      "\n",
      "From Scratch!, bests for سادرات are:\n",
      "[[1, 'سادات'], [2, 'خسارات'], [2, 'صادرات'], [2, 'سارا'], [2, 'ادات']] \n",
      "\n",
      "From Scratch!, bests for فوتکال are:\n",
      "[[2, 'فوتبال'], [2, 'فوتسال'], [3, 'فوت'], [3, 'فواصل'], [3, 'تکامل']] \n",
      "\n",
      "From Scratch!, bests for مسابغاط are:\n",
      "[[3, 'سابا'], [4, 'مساال'], [4, 'مسابقات'], [4, 'محابا'], [4, 'سابقا']] \n",
      "\n",
      "From Scratch!, bests for وازدات are:\n",
      "[[2, 'واداشت'], [2, 'موازات'], [2, 'واردات'], [2, 'ادات'], [3, 'وزارت']] \n",
      "\n",
      "From Scratch!, bests for مشرکت are:\n",
      "[[1, 'شرکت'], [1, 'مشارکت'], [1, 'مشرک'], [2, 'مشترک'], [2, 'شرکتی']] \n",
      "\n",
      "From Scratch!, bests for کشوور are:\n",
      "[[1, 'کشور'], [2, 'شور'], [2, 'کشوری'], [2, 'کشورش'], [2, 'کور']] \n",
      "\n",
      "From Scratch!, bests for منجلسه are:\n",
      "[[2, 'مجلس'], [2, 'جلسه'], [2, 'مجله'], [2, 'منجمله'], [3, 'مجسمه']] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['اختصاص', 'اقتصاد', 'اختصار', 'اقتصادی', 'افتاد'],\n",
       " ['سادات', 'خسارات', 'صادرات', 'سارا', 'ادات'],\n",
       " ['فوتبال', 'فوتسال', 'فوت', 'فواصل', 'تکامل'],\n",
       " ['سابا', 'مساال', 'مسابقات', 'محابا', 'سابقا'],\n",
       " ['واداشت', 'موازات', 'واردات', 'ادات', 'وزارت'],\n",
       " ['شرکت', 'مشارکت', 'مشرک', 'مشترک', 'شرکتی'],\n",
       " ['کشور', 'شور', 'کشوری', 'کشورش', 'کور'],\n",
       " ['مجلس', 'جلسه', 'مجله', 'منجمله', 'مجسمه']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_candidates = []\n",
    "for w in words:\n",
    "  print(f'From Scratch!, bests for {w} are:')\n",
    "  tmp = spell.predict_from_scratch(string = w , top_bests = 5 )\n",
    "  correction_candidates.append(tmp)\n",
    "  print(tmp,'\\n')\n",
    "\n",
    "candidate_words = []\n",
    "for sett in correction_candidates:\n",
    "  tmp = []\n",
    "  for can in sett:\n",
    "    tmp.append(can[1])\n",
    "  candidate_words.append(tmp)\n",
    "candidate_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzS1IzJ8-Q9O"
   },
   "source": [
    "## Q6 (Incorporate Bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MdWHKwQLqqY"
   },
   "outputs": [],
   "source": [
    "accum_bigrams = accum_ngram([parsi.clean_tokenize(x) for x in all], n_gram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mlg1nqj6ajLI",
    "outputId": "40d988f5-b3c5-482e-d999-df8e140b4d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on Bigrams, best word:اقتصادی witg prob:2.3056409667754882e-11\n",
      "\n",
      "Based on Bigrams, best word:صادرات witg prob:1.390483839012269e-07\n",
      "\n",
      "Based on Bigrams, best word:فوتبال witg prob:2.0465085532346898e-06\n",
      "\n",
      "Based on Bigrams, best word:مسابقات witg prob:7.464697729422069e-11\n",
      "\n",
      "Based on Bigrams, best word:واردات witg prob:2.8311229279674023e-10\n",
      "\n",
      "Based on Bigrams, best word:مشارکت witg prob:6.989758056699232e-08\n",
      "\n",
      "Based on Bigrams, best word:کشور witg prob:2.1509679954252117e-08\n",
      "\n",
      "Based on Bigrams, best word:مجلس witg prob:1.7246005873001862e-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question6_handler(candidate_words,bigrams_sent,accum_bigrams,mu = 0.1,Pbg = (1/spell.vocab_len))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xrljmN0LrOV"
   },
   "source": [
    "# Further Developments notes:\n",
    "\n",
    "Better DS for \"predict\" function from \"SpellCorrection\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IacoEBwYL43J",
    "outputId": "2f8f9584-645c-481e-d4b9-6d1bcf9f6a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                         Type                          Data/Info\n",
      "------------------------------------------------------------------------\n",
      "FarsiParsi                       type                          <class '__main__.FarsiParsi'>\n",
      "SpellCorrection                  type                          <class '__main__.SpellCorrection'>\n",
      "accum_bigrams                    dict                          n=52485\n",
      "accum_ngram                      function                      <function accum_ngram at 0x7f5e3ece0e60>\n",
      "all                              list                          n=814\n",
      "all_cooked                       list                          n=814\n",
      "bigrams_sent                     list                          n=8\n",
      "can                              list                          n=2\n",
      "candidate_words                  list                          n=8\n",
      "candidates                       list                          n=8\n",
      "char                             str                           و\n",
      "chars                            dict                          n=39\n",
      "cls                              function                      <function clear_output at 0x7f5e6f148c20>\n",
      "correction_candidates            list                          n=8\n",
      "count_finder                     function                      <function count_finder at 0x7f5e3ecd9cb0>\n",
      "d                                dict                          n=52484\n",
      "dam_lev                          builtin_function_or_method    <built-in function damerau_levenshtein>\n",
      "hazm                             module                        <module 'hazm' from '/usr<...>ckages/hazm/__init__.py'>\n",
      "i                                int                           38\n",
      "k                                str                           ‌\n",
      "lev                              builtin_function_or_method    <built-in function levenshtein>\n",
      "lst                              list                          n=1748\n",
      "np                               module                        <module 'numpy' from '/us<...>kages/numpy/__init__.py'>\n",
      "occurance_drichlet_probability   function                      <function occurance_drich<...>bility at 0x7f5e3ea60b90>\n",
      "osa                              builtin_function_or_method    <built-in function optimal_string_alignment>\n",
      "parsi                            FarsiParsi                    <__main__.FarsiParsi object at 0x7f5e3ea4c790>\n",
      "parsivar                         module                        <module 'parsivar' from '<...>es/parsivar/__init__.py'>\n",
      "pd                               module                        <module 'pandas' from '/u<...>ages/pandas/__init__.py'>\n",
      "per_eng_dict                     dict                          n=39\n",
      "punctuation                      str                           !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "question6_handler                function                      <function question6_handler at 0x7f5e1e7b1560>\n",
      "re                               module                        <module 're' from '/usr/lib/python3.7/re.py'>\n",
      "sample                           method                        <bound method Random.samp<...>bject at 0x55c90ffba120>>\n",
      "sent                             list                          n=6\n",
      "sent1                            list                          n=6\n",
      "sent2                            list                          n=6\n",
      "sett                             list                          n=5\n",
      "spell                            SpellCorrection               <__main__.SpellCorrection<...>object at 0x7f5e3e931b50>\n",
      "test_df                          DataFrame                             id               <...>\\n\\n[82 rows x 2 columns]\n",
      "tmp                              list                          n=5\n",
      "tmp1                             list                          n=732\n",
      "tok                              str                           پونزو\n",
      "train_df                         DataFrame                              id              <...>n\\n[732 rows x 2 columns]\n",
      "v                                int                           1\n",
      "vocab_lst                        list                          n=20883\n",
      "w                                str                           منجلسه\n",
      "words                            list                          n=8\n",
      "x                                list                          n=39\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW4_MohammadrezaArdestani.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
