{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewNDAy3exTvp"
   },
   "source": [
    "# Conversational Chatbot with a sequence-to-sequence Transformer\n",
    "\n",
    "**Reference:** [fchollet](https://twitter.com/fchollet)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dak_-6MouK0S",
    "outputId": "cb935b7c-4cd1-4a7b-92d6-7d33828bdd4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cUOShYYxTvu"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we'll build a sequence-to-sequence Transformer model, which\n",
    "we'll train on a conversational chatbot task.\n",
    "\n",
    "You'll learn how to:\n",
    "\n",
    "- Vectorize text using the Keras `TextVectorization` layer.\n",
    "- Implement a `TransformerEncoder` layer, a `TransformerDecoder` layer,\n",
    "and a `PositionalEmbedding` layer.\n",
    "- Prepare data for training a sequence-to-sequence model.\n",
    "- - Use the trained model to generate response sentence in a conversational chatbot\n",
    "input sentences (sequence-to-sequence inference).\n",
    "\n",
    "The code featured here is adapted from the book\n",
    "[Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n",
    "(chapter 11: Deep learning for text).\n",
    "The present example is fairly barebones, so for detailed explanations of\n",
    "how each building block works, as well as the theory behind Transformers,\n",
    "I recommend reading the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmfg1PQ9xTvu"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6MgRHMPxTvv"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BinwcsqfM40"
   },
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "808G4THdnm_6"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "  sentence = sentence.lower().strip()\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "  # removing contractions\n",
    "  sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
    "  sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
    "  sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
    "  sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
    "  sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
    "  sentence = re.sub(r\"what's\", \"what is\", sentence)\n",
    "  sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
    "  sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
    "  sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "  sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "  sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "  sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
    "  sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
    "  sentence = re.sub(r\"n't\", \" not\", sentence)\n",
    "  sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
    "  sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YA85PCynt28V"
   },
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "\n",
    "  text = []\n",
    "  with open(path, 'r') as f:\n",
    "    for line in f:\n",
    "      line = line.rstrip()\n",
    "      # print(line)\n",
    "      res = line.split('__eou__')\n",
    "\n",
    "      for i in range(0, len(res)-2):\n",
    "        sent_1 = preprocess_sentence(res[i])\n",
    "        sent_2 = preprocess_sentence(res[i+1])\n",
    "        text.append(sent_1)\n",
    "        text.append(\"[start] \" + sent_2 + \" [end]\")\n",
    "  \n",
    "  return text\n",
    "\n",
    "train_text = read_dataset('/content/drive/MyDrive/dataset/daily dialog/train/dialogues_train.txt')\n",
    "test_text = read_dataset('/content/drive/MyDrive/dataset/daily dialog/test/dialogues_test.txt')\n",
    "valid_text = read_dataset('/content/drive/MyDrive/dataset/daily dialog/validation/dialogues_validation.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGBzPjccvSa0"
   },
   "outputs": [],
   "source": [
    "dataset = train_text + test_text + valid_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lYjtkqCwoE-"
   },
   "outputs": [],
   "source": [
    "index = len(train_text)\n",
    "train_input = [train_text[i] for i in range(0,index) if i % 2 == 0]\n",
    "train_output = [train_text[i] for i in range(0,index) if i % 2 == 1]\n",
    "train_pairs = [(train, test) for train, test in zip(train_input, train_output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ve4nIJgixxU-"
   },
   "outputs": [],
   "source": [
    "index = len(test_text)\n",
    "test_input = [test_text[i] for i in range(0,index) if i % 2 == 0]\n",
    "test_output = [test_text[i] for i in range(0,index) if i % 2 == 1]\n",
    "test_pairs = [(train, test) for train, test in zip(test_input, test_output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbyqjVOOx3hi"
   },
   "outputs": [],
   "source": [
    "index = len(valid_text)\n",
    "val_input = [valid_text[i] for i in range(0,index) if i % 2 == 0]\n",
    "val_output = [valid_text[i] for i in range(0,index) if i % 2 == 1]\n",
    "val_pairs = [(train, test) for train, test in zip(val_input, val_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECP_LoVmxTv2"
   },
   "source": [
    "## Vectorizing the text data\n",
    "\n",
    "We'll use a instance of the `TextVectorization` layer to vectorize the text\n",
    "data,\n",
    "that is to say, to turn the original strings into integer sequences\n",
    "where each integer represents the index of a word in a vocabulary.\n",
    "\n",
    "We use the default string standardization (strip punctuation characters)\n",
    "and splitting scheme (split on whitespace).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7R3hoTFLxTv3"
   },
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "strip_chars_one = string.punctuation\n",
    "strip_chars_one = strip_chars_one.replace(\"?\", \"\")\n",
    "strip_chars_one = strip_chars_one.replace(\"!\", \"\")\n",
    "strip_chars_one = strip_chars_one.replace(\".\", \"\")\n",
    "strip_chars_one = strip_chars_one.replace(\",\", \"\")\n",
    "\n",
    "def custom_standardization_encoder(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars_one), \"\")\n",
    "\n",
    "\n",
    "inp_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
    "    standardize = custom_standardization_encoder\n",
    ")\n",
    "\n",
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "strip_chars = strip_chars.replace(\"?\", \"\")\n",
    "strip_chars = strip_chars.replace(\"!\", \"\")\n",
    "strip_chars = strip_chars.replace(\".\", \"\")\n",
    "strip_chars = strip_chars.replace(\",\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "out_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length+1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "index = len(dataset)\n",
    "inp_texts = [dataset[i] for i in range(0,index) if i % 2 == 0]\n",
    "out_texts = [dataset[i] for i in range(0,index) if i % 2 == 1]\n",
    "\n",
    "\n",
    "inp_vectorization.adapt(inp_texts)\n",
    "out_vectorization.adapt(out_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LwuKcgVxTv4"
   },
   "source": [
    "Next, we'll format our datasets.\n",
    "\n",
    "At each training step, the model will seek to predict target words N+1 (and beyond)\n",
    "using the source sentence and the target words 0 to N.\n",
    "\n",
    "As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n",
    "\n",
    "- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n",
    "`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n",
    "that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n",
    "- `target` is the target sentence offset by one step:\n",
    "it provides the next words in the target sentence -- what the model will try to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAWYLGK7RmNx"
   },
   "source": [
    "## Decoder input:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=10oBrq4YjCuB-bw2ovUp-hCJK0p_8gV2C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eptR75PKxTv5"
   },
   "outputs": [],
   "source": [
    "def format_dataset(inp, out):\n",
    "    inp = enc_vectorization(inp)\n",
    "    out = dec_vectorization(out)\n",
    "    return ({\"encoder_inputs\": inp, \"decoder_inputs\": out[:, :-1],}, out[:, 1:])\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    inp_texts, out_texts = zip(*pairs)\n",
    "    inp_texts = list(inp_texts)\n",
    "    out_texts = list(out_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inp_texts, out_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "test_ds = make_dataset(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVAHSg4moi5r",
    "outputId": "3cc77894-438b-4c2d-f2d9-f0ea0b768e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CacheDataset shapes: ({encoder_inputs: (None, 20), decoder_inputs: (None, 20)}, (None, 20)), types: ({encoder_inputs: tf.int64, decoder_inputs: tf.int64}, tf.int64)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCjbgTwoxTv5"
   },
   "source": [
    "Let's take a quick look at the sequence shapes\n",
    "(we have batches of 64 pairs, and all sequences are 20 steps long):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEvogWfzxTv6",
    "outputId": "dc6416a6-280e-4656-f5e4-5af8d788064e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    \n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpRVnfihxTv6"
   },
   "source": [
    "## Building the model\n",
    "\n",
    "Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n",
    "and a `TransformerDecoder` chained together. To make the model aware of word order,\n",
    "we also use a `PositionalEmbedding` layer.\n",
    "\n",
    "The source sequence will be pass to the `TransformerEncoder`,\n",
    "which will produce a new representation of it.\n",
    "This new representation will then be passed\n",
    "to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n",
    "The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n",
    "\n",
    "A key detail that makes this possible is causal masking\n",
    "(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n",
    "The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n",
    "sure that it only uses information from target tokens 0 to N when predicting token N+1\n",
    "(otherwise, it could use information from the future, which would\n",
    "result in a model that cannot be used at inference time).\n",
    "\n",
    "## Model Architecture:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1_BX9YvY-E_7ttPOiY2wIOG6KW4FkmdAm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGLP3RXctWu8"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_kFlKPHxTv7"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPBKEDB1tbsV"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_hLVJtvRPLi"
   },
   "source": [
    "## Attention:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=158scEbQNyJg1TxMhrDCWAN925wRCHjfe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMCqAj2XxTv8"
   },
   "source": [
    "Next, we assemble the end-to-end model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwo9Fr10xTv8"
   },
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC_2P8XsxTv9"
   },
   "source": [
    "## Training our model\n",
    "\n",
    "We'll use accuracy as a quick way to monitor training progress on the validation data.\n",
    "Note that machine translation typically uses BLEU scores as well as other metrics, rather than accuracy.\n",
    "\n",
    "Here we only train for 1 epoch, but to get the model to actually converge\n",
    "you should train for at least 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPqL-g2UxTv-"
   },
   "outputs": [],
   "source": [
    "epochs = 30 \n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtA5kRnDxTv_"
   },
   "source": [
    "## Decoding test sentences\n",
    "\n",
    "Finally, let's demonstrate how to reply to a conversation.\n",
    "We simply feed into the model the vectorized input sentence\n",
    "as well as the target token `\"[start]\"`, then we repeatedly generated the next token, until\n",
    "we hit the token `\"[end]\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lq7E2YvqxTv_"
   },
   "outputs": [],
   "source": [
    "out_vocab = out_vectorization.get_vocabulary()\n",
    "out_index_lookup = dict(zip(range(len(out_vocab)), out_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = inp_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = out_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = out_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "          break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAEUMk8sECm6"
   },
   "outputs": [],
   "source": [
    "input_sentences = ['bob ! i hear your team won the match .',\n",
    "                   'how do you do ?',\n",
    "                   'do you think you are introverted or extroverted ?',\n",
    "                   'yes , you are right . after all , the quality of your air conditioners is good . the only problem is price .',\n",
    "                   'what ? he cannot do this to you .',\n",
    "                   'thank you , lisa .',\n",
    "                   'oh , she can make her own decisions .',\n",
    "                   'but if i do not pass , will you call me ?',\n",
    "                   'what happened , john ?',\n",
    "                   'nice to meet you , mr . wilson .',\n",
    "                   'hello , is sue there ?',\n",
    "                   'hi ! i am happy you could make it .',\n",
    "                   'what foods do you eat now ?',\n",
    "                   'ok . is the plane on schedule ?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nd0-DzM5Fj_1"
   },
   "outputs": [],
   "source": [
    "for input_sentence in input_sentences:\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print('***************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DXxqQzrAzzP"
   },
   "outputs": [],
   "source": [
    "for _ in range(30):\n",
    "    input_sentence = random.choice(test_input)\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print('***************')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Conversational_Chatbot_with_Transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
