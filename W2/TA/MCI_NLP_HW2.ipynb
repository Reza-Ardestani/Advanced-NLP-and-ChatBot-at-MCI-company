{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6_DyCHqOa9g"
   },
   "source": [
    "# Preprocess\n",
    "*Downlaod* data and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bR5OZ2WAp3Ka",
    "outputId": "0f28ba69-a4a3-4e7f-f4c0-7f1668bd499c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1qCVYpb67RuzUbyrJ3w-ohtCf-tzZKTal\n",
      "To: /content/train.txt\n",
      "9.87MB [00:00, 21.3MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1dW5SkCYIFbXmNe3xKv4EhrLPDPlXyIDy\n",
      "To: /content/test.txt\n",
      "100% 5.80k/5.80k [00:00<00:00, 12.3MB/s]\n",
      "بسیارخوری وکامرانی\n",
      " 2438\n",
      "هنگام مکافات\n",
      " 31304\n",
      "حضرت ستارخان\n",
      " 57052\n",
      "سلام چطوری\n",
      " 67800\n",
      "گلبنان ممتحن\n",
      " 74076\n",
      "همدست وهمداستان\n",
      " 78314\n",
      "ای نازنین\n",
      " 96936\n",
      "قصر چلستون\n",
      " 101252\n",
      "بسیار سخن\n",
      " 107340\n",
      "اندر آذربایجان\n",
      " 108670\n",
      "نپذیرند یکی\n",
      " 118562\n",
      "بیشتر کن\n",
      " 140401\n",
      "بیچاره         رعیت\n",
      " 151833\n",
      "اندرگلستان ما\n",
      " 152975\n",
      "ناگاه صبا\n",
      " 159903\n",
      "پرشرر کن\n",
      " 167759\n",
      "همدرد ندارد\n",
      " 175501\n",
      "برملت خوبشتن\n",
      " 184457\n",
      "188894 188876\n",
      "108 108\n"
     ]
    }
   ],
   "source": [
    "#https://drive.google.com/file/d/1qCVYpb67RuzUbyrJ3w-ohtCf-tzZKTal/view?usp=sharing\n",
    "#https://drive.google.com/file/d/1dW5SkCYIFbXmNe3xKv4EhrLPDPlXyIDy/view?usp=sharing\n",
    "\n",
    "\n",
    "!gdown --id 1qCVYpb67RuzUbyrJ3w-ohtCf-tzZKTal\n",
    "!gdown --id 1dW5SkCYIFbXmNe3xKv4EhrLPDPlXyIDy\n",
    "\n",
    "\n",
    "import io\n",
    "import re\n",
    "\n",
    "def file_clear(in_f, out_f):\n",
    "    inputFile = io.open(in_f, mode=\"r\", encoding=\"utf-8\") \n",
    "    outputFile = io.open(out_f, mode=\"w\", encoding=\"utf-8\") \n",
    "\n",
    "    updated=[]\n",
    "    Lines = inputFile.readlines()\n",
    "    for i, line in enumerate(Lines):\n",
    "        newline= re.sub(' +', ' ', line.strip())\n",
    "        if len(line.split()) >2:    \n",
    "            updated.append(newline+\"\\n\")\n",
    "        else:\n",
    "            print(line,i)    \n",
    "    outputFile.writelines(updated)\n",
    "    print( len(Lines), len(updated))\n",
    "    inputFile.__exit__()\n",
    "    outputFile.__exit__()\n",
    "#updated\n",
    "\n",
    "file_clear('train.txt', 'cleaned_train.txt')\n",
    "file_clear('test.txt', 'cleaned_test.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQ3a2cfMC9Sn"
   },
   "source": [
    "# Model Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGwQYaxx4Ur_"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import math\n",
    "import nltk\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_QzRFyPP5QZ"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "SOS = \"<s> \"\n",
    "EOS = \"</s>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "def add_sentence_tokens(sentences, n):\n",
    "    \"\"\"Wrap each sentence in SOS and EOS tokens.\n",
    "    For n >= 2, n-1 SOS tokens are added, otherwise only one is added.\n",
    "    Args:\n",
    "        sentences (list of str): the sentences to wrap.\n",
    "        n (int): order of the n-gram model which will use these sentences.\n",
    "    Returns:\n",
    "        List of sentences with SOS and EOS tokens wrapped around them.\n",
    "    \"\"\"\n",
    "    sos = SOS * (n-1) if n > 1 else SOS\n",
    "    return ['{}{} {}'.format(sos, s, EOS) for s in sentences]\n",
    "\n",
    "def replace_singletons(tokens):\n",
    "    \"\"\"Replace tokens which appear only once in the corpus with <UNK>.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list of str): the tokens comprising the corpus.\n",
    "    Returns:\n",
    "        The same list of tokens with each singleton replaced by <UNK>.\n",
    "    \n",
    "    \"\"\"\n",
    "    vocab = nltk.FreqDist(tokens)\n",
    "    return [token if vocab[token] > 1 else UNK for token in tokens]\n",
    "\n",
    "def preprocess(sentences, n):\n",
    "    \"\"\"Add SOS/EOS/UNK tokens to given sentences and tokenize.\n",
    "    Args:\n",
    "        sentences (list of str): the sentences to preprocess.\n",
    "        n (int): order of the n-gram model which will use these sentences.\n",
    "    Returns:\n",
    "        The preprocessed sentences, tokenized by words.\n",
    "    \"\"\"\n",
    "    sentences = add_sentence_tokens(sentences, n)\n",
    "    tokens = ' '.join(sentences).split(' ')\n",
    "    tokens = replace_singletons(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "#from preprocess import preprocess\n",
    "\n",
    "\n",
    "def load_data(train_path, test_path=\"\"):\n",
    "    \"\"\"Load train and test corpora from a directory.\n",
    "    Directory must contain two files: train.txt and test.txt.\n",
    "    Newlines will be stripped out. \n",
    "    Args:\n",
    "        data_dir (Path) -- pathlib.Path of the directory to use. \n",
    "    Returns:\n",
    "        The train and test sets, as lists of sentences.\n",
    "    \"\"\"\n",
    "    # train_path = data_dir.joinpath('train.txt').absolute().as_posix()\n",
    "    # test_path  = data_dir.joinpath('test.txt').absolute().as_posix()\n",
    "\n",
    "    with open(train_path, 'r') as f:\n",
    "        train = [l.strip() for l in f.readlines()]\n",
    "\n",
    "    if test_path != \"\":  \n",
    "        with open(test_path, 'r') as f:\n",
    "            test = [l.strip() for l in f.readlines()]\n",
    "    else:\n",
    "        return train        \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBH_-yPkDFVX"
   },
   "outputs": [],
   "source": [
    "class LanguageModel(object):\n",
    "    \"\"\"An n-gram language model trained on a given corpus.\n",
    "    \n",
    "    For a given n and given training corpus, constructs an n-gram language\n",
    "    model for the corpus by:\n",
    "    1. preprocessing the corpus (adding SOS/EOS/UNK tokens)\n",
    "    2. calculating (smoothed) probabilities for each n-gram\n",
    "    Also contains methods for calculating the perplexity of the model\n",
    "    against another corpus, and for generating sentences.\n",
    "    Args:\n",
    "        train_data (list of str): list of sentences comprising the training corpus.\n",
    "        n (int): the order of language model to build (i.e. 1 for unigram, 2 for bigram, etc.).\n",
    "        laplace (int): lambda multiplier to use for laplace smoothing (default 1 for add-1 smoothing).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_data, n, laplace=1):\n",
    "        self.n = n\n",
    "        self.laplace = laplace\n",
    "        self.tokens = preprocess(train_data, n)\n",
    "        self.vocab  = nltk.FreqDist(self.tokens)\n",
    "        self.model  = self._create_model()\n",
    "        self.masks  = list(reversed(list(product((0,1), repeat=n))))\n",
    "\n",
    "    def _smooth(self):\n",
    "        \"\"\"Apply Laplace smoothing to n-gram frequency distribution.\n",
    "        \n",
    "        Here, n_grams refers to the n-grams of the tokens in the training corpus,\n",
    "        while m_grams refers to the first (n-1) tokens of each n-gram.\n",
    "        Returns:\n",
    "            dict: Mapping of each n-gram (tuple of str) to its Laplace-smoothed \n",
    "            probability (float).\n",
    "        \"\"\"\n",
    "        vocab_size = len(self.vocab)\n",
    "\n",
    "        n_grams = nltk.ngrams(self.tokens, self.n)\n",
    "        n_vocab = nltk.FreqDist(n_grams)\n",
    "\n",
    "        m_grams = nltk.ngrams(self.tokens, self.n-1)\n",
    "        m_vocab = nltk.FreqDist(m_grams)\n",
    "\n",
    "        def smoothed_count(n_gram, n_count):\n",
    "            m_gram = n_gram[:-1]\n",
    "            m_count = m_vocab[m_gram]\n",
    "            return (n_count + self.laplace) / (m_count + self.laplace * vocab_size)\n",
    "\n",
    "        return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n",
    "\n",
    "    def _create_model(self):\n",
    "        \"\"\"Create a probability distribution for the vocabulary of the training corpus.\n",
    "        \n",
    "        If building a unigram model, the probabilities are simple relative frequencies\n",
    "        of each token with the entire corpus.\n",
    "        Otherwise, the probabilities are Laplace-smoothed relative frequencies.\n",
    "        Returns:\n",
    "            A dict mapping each n-gram (tuple of str) to its probability (float).\n",
    "        \"\"\"\n",
    "        if self.n == 1:\n",
    "            num_tokens = len(self.tokens)\n",
    "            return { (unigram,): count / num_tokens for unigram, count in self.vocab.items() }\n",
    "        else:\n",
    "            return self._smooth()\n",
    "\n",
    "    def _convert_oov(self, ngram):\n",
    "        \"\"\"Convert, if necessary, a given n-gram to one which is known by the model.\n",
    "        Starting with the unmodified ngram, check each possible permutation of the n-gram\n",
    "        with each index of the n-gram containing either the original token or <UNK>. Stop\n",
    "        when the model contains an entry for that permutation.\n",
    "        This is achieved by creating a 'bitmask' for the n-gram tuple, and swapping out\n",
    "        each flagged token for <UNK>. Thus, in the worst case, this function checks 2^n\n",
    "        possible n-grams before returning.\n",
    "        Returns:\n",
    "            The n-gram with <UNK> tokens in certain positions such that the model\n",
    "            contains an entry for it.\n",
    "        \"\"\"\n",
    "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n",
    "\n",
    "        ngram = (ngram,) if type(ngram) is str else ngram\n",
    "        for possible_known in [mask(ngram, bitmask) for bitmask in self.masks]:\n",
    "            if possible_known in self.model:\n",
    "                return possible_known\n",
    "\n",
    "    def perplexity(self, test_data):\n",
    "        \"\"\"Calculate the perplexity of the model against a given test corpus.\n",
    "        \n",
    "        Args:\n",
    "            test_data (list of str): sentences comprising the training corpus.\n",
    "        Returns:\n",
    "            The perplexity of the model as a float.\n",
    "        \n",
    "        \"\"\"\n",
    "        test_tokens = preprocess(test_data, self.n)\n",
    "        test_ngrams = nltk.ngrams(test_tokens, self.n)\n",
    "        N = len(test_tokens)\n",
    "\n",
    "        known_ngrams  = (self._convert_oov(ngram) for ngram in test_ngrams)\n",
    "        probabilities = [self.model[ngram] for ngram in known_ngrams]\n",
    "        \n",
    "        return math.exp((-1/N) * sum(map(math.log, probabilities)))\n",
    "\n",
    "    def _best_candidate(self, prev, i, without=[]):\n",
    "        \"\"\"Choose the most likely next token given the previous (n-1) tokens.\n",
    "        If selecting the first word of the sentence (after the SOS tokens),\n",
    "        the i'th best candidate will be selected, to create variety.\n",
    "        If no candidates are found, the EOS token is returned with probability 1.\n",
    "        Args:\n",
    "            prev (tuple of str): the previous n-1 tokens of the sentence.\n",
    "            i (int): which candidate to select if not the most probable one.\n",
    "            without (list of str): tokens to exclude from the candidates list.\n",
    "        Returns:\n",
    "            A tuple with the next most probable token and its corresponding probability.\n",
    "        \"\"\"\n",
    "        blacklist  = [\"<UNK>\"] + without\n",
    "        candidates = ((ngram[-1],prob) for ngram,prob in self.model.items() if ngram[:-1]==prev)\n",
    "        candidates = filter(lambda candidate: candidate[0] not in blacklist, candidates)\n",
    "        candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)\n",
    "        if len(candidates) == 0:\n",
    "            return (\"</s>\", 1)\n",
    "        else:\n",
    "            return candidates[0 if prev != () and prev[-1] != \"<s>\" else i]\n",
    "     \n",
    "    def generate_sentences(self, num, min_len=12, max_len=24):\n",
    "        \"\"\"Generate num random sentences using the language model.\n",
    "        Sentences always begin with the SOS token and end with the EOS token.\n",
    "        While unigram model sentences will only exclude the UNK token, n>1 models\n",
    "        will also exclude all other words already in the sentence.\n",
    "        Args:\n",
    "            num (int): the number of sentences to generate.\n",
    "            min_len (int): minimum allowed sentence length.\n",
    "            max_len (int): maximum allowed sentence length.\n",
    "        Yields:\n",
    "            A tuple with the generated sentence and the combined probability\n",
    "            (in log-space) of all of its n-grams.\n",
    "        \"\"\"\n",
    "        for i in range(num):\n",
    "            sent, prob = [\"<s>\"] * max(1, self.n-1), 1\n",
    "            while sent[-1] != \"</s>\":\n",
    "                prev = () if self.n == 1 else tuple(sent[-(self.n-1):])\n",
    "                blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n",
    "                next_token, next_prob = self._best_candidate(prev, i, without=blacklist)\n",
    "                sent.append(next_token)\n",
    "                prob *= next_prob\n",
    "                \n",
    "                if len(sent) >= max_len:\n",
    "                    sent.append(\"</s>\")\n",
    "\n",
    "            yield ' '.join(sent), -1/math.log(prob)\n",
    "\n",
    "    def complete_sentence(self, phrase, missing_count): #min_len=12, max_len=24):\n",
    "        sent, prob = [\"<s>\"] * max(1, self.n-1), 1\n",
    "        sent = sent + [ x.strip() for x in phrase.split() if len(x)>1]\n",
    "        for i in range(missing_count):\n",
    "            prev = () if self.n == 1 else tuple(sent[-(self.n-1):])\n",
    "            blacklist = sent + [\"</s>\"] #([\"</s>\"] if len(sent) < min_len else [])\n",
    "            next_token, next_prob = self._best_candidate(prev, i, without=blacklist)\n",
    "            sent.append(next_token)\n",
    "            prob *= next_prob\n",
    "            \n",
    "            # if len(sent) >= max_len:\n",
    "            #     sent.append(\"</s>\")\n",
    "        sent = sent + [\"</s>\"] * max(1, self.n-1)\n",
    "        return ' '.join(sent), -1/math.log(prob)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nWAQo7GA1dRu",
    "outputId": "13a950c1-19f2-4d7d-ae17-b9d02a76a0d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2-gram model...\n",
      "Vocabulary size: 27927\n",
      "Generating sentences...\n",
      "<s> که در آن را به دست و از تو بر سر </s> (0.02418)\n",
      "<s> به دست و از آن را ز تو در دل من </s> (0.02418)\n",
      "<s> از آن را به دست و در دل من ز تو </s> (0.02405)\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self, train_f=\"cleaned_train.txt\", test_f=\"cleaned_test.txt\",  n=2, laplace=.01, num=3):\n",
    "        self.train_f = train_f\n",
    "        self.test_f = test_f\n",
    "\n",
    "        self.n=n\n",
    "        self.laplace=laplace\n",
    "        self.num=num\n",
    "args = Args()\n",
    "\n",
    "\n",
    "# Load and prepare train/test data\n",
    "# data_path = Path(args.data)\n",
    "# train, test = load_data(data_path)\n",
    "train, test = load_data(Path(args.train_f),Path(args.test_f))\n",
    "\n",
    "print(\"Loading {}-gram model...\".format(args.n))\n",
    "lm = LanguageModel(train, args.n, laplace=args.laplace)\n",
    "print(\"Vocabulary size: {}\".format(len(lm.vocab)))\n",
    "\n",
    "print(\"Generating sentences...\")\n",
    "for sentence, prob in lm.generate_sentences(args.num):\n",
    "    print(\"{} ({:.5f})\".format(sentence, prob))\n",
    "\n",
    "# perplexity = lm.perplexity(test)\n",
    "# print(\"Model perplexity: {:.3f}\".format(perplexity))\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfpQU5PKOnxX"
   },
   "source": [
    "# Q1 Dirichlet Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GoewJ5weYR84",
    "outputId": "e7774a78-bcfe-49f8-b68f-37f3eef57e28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<s> چون مشک سیه بود مرا هر دو چشم من </s>', 0.17603589641442205)\n",
      "('<s> گر خورد سوگند هم آن را </s>', 0.26795928100828315)\n",
      "('<s> زانک نفس آشفته تر گردد از آن </s>', 0.3208489208062978)\n",
      "('<s> ازین زشت تر در جهان رنگ و </s>', 0.45793907493459973)\n",
      "('<s> دوست در خانه ما گرد و از </s>', 0.12235655018702388)\n",
      "('<s> شب است شمع شراب و </s>', 0.36522037121905526)\n"
     ]
    }
   ],
   "source": [
    "sents_to_comp=[ (\"چون مشک سیه بود مرا هر دو\", 2),\n",
    "                (\"گر خورد سوگند هم آن\", 1),\n",
    "                (\"زانک نفس آشفته تر گردد از\",1),\n",
    "                (\"ازین زشت تر در جهان رنگ\",1),\n",
    "                (\"دوست در خانه و ما گرد\",2),\n",
    "                (\"شب است و شمع و شراب و\",1)\n",
    "]\n",
    "\n",
    "for phrase, missing_count in sents_to_comp:\n",
    "    completed = lm.complete_sentence(phrase, missing_count)\n",
    "    print(completed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QBZ84HG-NNc"
   },
   "source": [
    "# Q2 Perpelexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KbhSHkAV-MoD",
    "outputId": "9ad7d993-03a6-4b55-bdce-4f0a473f70a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.69355828796971"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.perplexity(test)\n",
    "#lm.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F928kRkuOtBt"
   },
   "source": [
    "#Q3 Neural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V89WwK-9OzAo"
   },
   "outputs": [],
   "source": [
    "#Remove Big Prev Model \n",
    "lm = None\n",
    "\n",
    "#Get Words\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams    \n",
    "import collections\n",
    "\n",
    "def compute_freq(inputfile, nu):\n",
    "    textfile = io.open(inputfile, mode=\"r\", encoding=\"utf-8\")\n",
    "    Lines = textfile.readlines()\n",
    "    textfile.__exit__()            \n",
    "    ngramfdist = FreqDist()\n",
    " \n",
    "    for i,line in enumerate(Lines):\n",
    "        tokens = line.strip().split(' ')\n",
    "        if len(tokens)>nu:\n",
    "            # ngram_ = ngrams(tokens, nu)\n",
    "            # print(i+1,\" \",line)\n",
    "            ngram_ = ngrams(tokens, nu)\n",
    "            ngramfdist.update(ngram_)\n",
    "    return ngramfdist\n",
    "\n",
    "words = compute_freq(\"cleaned_train.txt\",1)\n",
    "#words_dict = collections.Counter(ngrams)\n",
    "words = [i[0] for i in words]\n",
    "word2int = dict(enumerate(words))\n",
    "word2int = {word: ind for ind, word in word2int.items()}\n",
    "int2word = {value: key for key, value in word2int.items()}\n",
    "int2word.keys()\n",
    "\n",
    "vocab_size= len(int2word)\n",
    "\n",
    "#Get Trigrams\n",
    "def generateSeq(inputfile, nu):\n",
    "    textfile = io.open(inputfile, mode=\"r\", encoding=\"utf-8\")\n",
    "    Lines = textfile.readlines()\n",
    "    textfile.__exit__()            \n",
    "    sequences={}\n",
    "    seqIndex=0\n",
    "    for i,line in enumerate(Lines):\n",
    "        tokens = line.strip().split(' ')\n",
    "        if len(tokens)>nu:\n",
    "            ngrams_ = ngrams(tokens, nu)\n",
    "            for tuple_ in ngrams_:\n",
    "                t = list(tuple_)\n",
    "                sequences[seqIndex]= ([word2int[i] for i in t[:-1]], word2int[t[-1]] )    \n",
    "                seqIndex = seqIndex+1\n",
    "    return sequences\n",
    "\n",
    "sequences_dict = generateSeq(\"cleaned_train.txt\",3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIlDNlEwj0y4",
    "outputId": "d51a889b-81d8-4be4-c442-998a78cb3db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972.4722065925598\n",
      "835.3044457435608\n",
      "811.8480472564697\n",
      "792.6217665672302\n",
      "773.9694743156433\n",
      "754.6817936897278\n",
      "734.3321185112\n",
      "713.6036310195923\n",
      "694.2062292098999\n",
      "677.9491186141968\n",
      "665.4597520828247\n",
      "655.7763133049011\n",
      "647.8947401046753\n",
      "641.1842975616455\n",
      "635.1420650482178\n",
      "629.7935228347778\n",
      "624.8499727249146\n",
      "620.3488807678223\n",
      "616.0712938308716\n",
      "612.0496368408203\n",
      "608.3208708763123\n",
      "604.7394375801086\n",
      "601.3440775871277\n",
      "598.1088042259216\n",
      "594.9670538902283\n",
      "591.9797043800354\n",
      "589.0549421310425\n",
      "586.2892870903015\n",
      "583.5651874542236\n",
      "580.9411664009094\n",
      "578.401029586792\n",
      "575.863477230072\n",
      "573.5538420677185\n",
      "571.1129097938538\n",
      "568.8731560707092\n",
      "566.5863628387451\n",
      "564.4155249595642\n",
      "562.2446885108948\n",
      "560.1706924438477\n",
      "558.1298184394836\n",
      "556.1356468200684\n",
      "554.2393755912781\n",
      "552.268792629242\n",
      "550.3745546340942\n",
      "548.5836338996887\n",
      "546.7639651298523\n",
      "544.9467716217041\n",
      "543.1820521354675\n",
      "541.508894443512\n",
      "539.8328895568848\n",
      "538.2073760032654\n",
      "536.6298027038574\n",
      "535.0380930900574\n",
      "533.4468946456909\n",
      "531.9985151290894\n",
      "530.4819054603577\n",
      "528.9754271507263\n",
      "527.5605340003967\n",
      "526.1493396759033\n",
      "524.7065315246582\n",
      "523.3955373764038\n",
      "522.0511813163757\n",
      "520.7348155975342\n",
      "519.4446105957031\n",
      "518.1908736228943\n",
      "516.9053263664246\n",
      "515.6844172477722\n",
      "514.5231194496155\n",
      "513.2888336181641\n",
      "512.1435241699219\n",
      "511.0362491607666\n",
      "509.855429649353\n",
      "508.8052587509155\n",
      "507.6433901786804\n",
      "506.6161093711853\n",
      "505.54295110702515\n",
      "504.56165981292725\n",
      "503.5243992805481\n",
      "502.52984142303467\n",
      "501.56322479248047\n",
      "500.55074548721313\n",
      "499.5616478919983\n",
      "498.7545175552368\n",
      "497.7554044723511\n",
      "496.8758454322815\n",
      "495.9760584831238\n",
      "495.0588297843933\n",
      "494.2250518798828\n",
      "493.37214040756226\n",
      "492.5686779022217\n",
      "491.7106475830078\n",
      "490.9186758995056\n",
      "490.1237440109253\n",
      "489.30679416656494\n",
      "488.528724193573\n",
      "487.7621831893921\n",
      "487.03975439071655\n",
      "486.25167655944824\n",
      "485.55132007598877\n",
      "484.77430057525635\n",
      "[972.4722065925598, 835.3044457435608, 811.8480472564697, 792.6217665672302, 773.9694743156433, 754.6817936897278, 734.3321185112, 713.6036310195923, 694.2062292098999, 677.9491186141968, 665.4597520828247, 655.7763133049011, 647.8947401046753, 641.1842975616455, 635.1420650482178, 629.7935228347778, 624.8499727249146, 620.3488807678223, 616.0712938308716, 612.0496368408203, 608.3208708763123, 604.7394375801086, 601.3440775871277, 598.1088042259216, 594.9670538902283, 591.9797043800354, 589.0549421310425, 586.2892870903015, 583.5651874542236, 580.9411664009094, 578.401029586792, 575.863477230072, 573.5538420677185, 571.1129097938538, 568.8731560707092, 566.5863628387451, 564.4155249595642, 562.2446885108948, 560.1706924438477, 558.1298184394836, 556.1356468200684, 554.2393755912781, 552.268792629242, 550.3745546340942, 548.5836338996887, 546.7639651298523, 544.9467716217041, 543.1820521354675, 541.508894443512, 539.8328895568848, 538.2073760032654, 536.6298027038574, 535.0380930900574, 533.4468946456909, 531.9985151290894, 530.4819054603577, 528.9754271507263, 527.5605340003967, 526.1493396759033, 524.7065315246582, 523.3955373764038, 522.0511813163757, 520.7348155975342, 519.4446105957031, 518.1908736228943, 516.9053263664246, 515.6844172477722, 514.5231194496155, 513.2888336181641, 512.1435241699219, 511.0362491607666, 509.855429649353, 508.8052587509155, 507.6433901786804, 506.6161093711853, 505.54295110702515, 504.56165981292725, 503.5243992805481, 502.52984142303467, 501.56322479248047, 500.55074548721313, 499.5616478919983, 498.7545175552368, 497.7554044723511, 496.8758454322815, 495.9760584831238, 495.0588297843933, 494.2250518798828, 493.37214040756226, 492.5686779022217, 491.7106475830078, 490.9186758995056, 490.1237440109253, 489.30679416656494, 488.528724193573, 487.7621831893921, 487.03975439071655, 486.25167655944824, 485.55132007598877, 484.77430057525635]\n"
     ]
    }
   ],
   "source": [
    "#input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "import torch\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, sequence_dict):\n",
    "        'Initialization'\n",
    "        self.data_dict = sequence_dict\n",
    "        # self.labels = labels\n",
    "        # self.list_IDs = list_IDs\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.data_dict)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        # ID = self.list_IDs[index]\n",
    "\n",
    "        # # Load data and get label\n",
    "        # X = torch.load('data/' + ID + '.pt')\n",
    "        # y = self.labels[ID]\n",
    "        x,y = self.data_dict[index]\n",
    "        return tuple(x), y\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #embeds = self.embeddings(inputs).view((1, -1))\n",
    "        embeds= torch.cat((self.embeddings(inputs[0]),self.embeddings(inputs[1])),1)\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 1024*8,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "max_epochs = 100\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(sequences_dict)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(vocab_size, EMBEDDING_DIM, CONTEXT_SIZE).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(max_epochs):\n",
    "    total_loss = 0\n",
    "    for context, target in training_generator:\n",
    "\n",
    "        context_idxs = [context[0].to(device), context[1].to(device)] \n",
    "        model.zero_grad()\n",
    "\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        loss = loss_function(log_probs, target.to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(epoch, \" \",total_loss)\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!\n",
    "\n",
    "# To get the embedding of a particular word, e.g. \"beauty\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bIOXyD_NUZs5",
    "outputId": "01fb7b56-c55a-42da-ba86-ba4f43b7e42d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "چون مشک سیه بود مرا هر دو را به\n",
      "گر خورد سوگند هم آن را\n",
      "زانک نفس آشفته تر گردد از اجلال\n",
      "ازین زشت تر در جهان رنگ و\n",
      "دوست در خانه و ما گرد تو بر\n",
      "شب است و شمع و شراب و طعام\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sents_to_comp=[ (\"چون مشک سیه بود مرا هر دو\", 2),\n",
    "                (\"گر خورد سوگند هم آن\", 1),\n",
    "                (\"زانک نفس آشفته تر گردد از\",1),\n",
    "                (\"ازین زشت تر در جهان رنگ\",1),\n",
    "                (\"دوست در خانه و ما گرد\",2),\n",
    "                (\"شب است و شمع و شراب و\",1)\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "for sent, i2 in  sents_to_comp:\n",
    "    for i in range(i2):\n",
    "        sent_words= sent.strip().split()\n",
    "        context = [ torch.tensor([word2int[sent_words[-2]]]).to(device) , torch.tensor([word2int[sent_words[-1]]]).to(device)]\n",
    "        nextWordPreds = model(context)\n",
    "        nextWordID = np.argmax(nextWordPreds.detach().cpu()).item()\n",
    "        nextWord = int2word[nextWordID]\n",
    "        sent = sent + \" \"+ nextWord\n",
    "    print(sent)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "tQ3a2cfMC9Sn",
    "JsteSWYnOwGh",
    "VqCIYdE9PMXg",
    "q1Fn9eZC4IP4"
   ],
   "name": "MCI-NLP-HW2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
